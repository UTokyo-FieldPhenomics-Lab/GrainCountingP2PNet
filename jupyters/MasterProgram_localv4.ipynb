{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the excpet **Dataset.py** block from `gcp2pnet` module\n",
    "\n",
    "Success in training model\n",
    "\n",
    "Todo:\n",
    "\n",
    "- replace parse train_args and using this dateset and gcp2pnet.train.main() to run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from scipy import spatial\n",
    "import networkx as nx\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import glob\n",
    "import scipy.io as io\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import gcp2pnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimming_size               = 256\n",
    "padding_size                = 0\n",
    "learning_rate               = 1e-3 #learning rate for backgrond model\n",
    "learning_rate_2             = 1e-4 #learning rate for detection head\n",
    "\n",
    "label_dict = {'Fill': 1, '平べったいけど沈む': 1, '平べったくて浮く': 2, '詰まっているけど浮く': 2, 'Unfill': 2} \n",
    "label_type_count = 2+1\n",
    "\n",
    "ROOT_DIR = r\"/home/hwang/d/GoogleDrive/howcanoewang@gmail.com/SharedWithMe/GrainCounting\"\n",
    "\n",
    "training_data_root_dir      = f\"{ROOT_DIR}/AnalysisData_small/\"\n",
    "training_data_root_dir2     = f\"{ROOT_DIR}/AnalysisData/\"\n",
    "program_files_root_dir      = f\"{ROOT_DIR}/programs/\"\n",
    "net_code_dir                = f\"{program_files_root_dir}/CrowdCounting-P2PNet\"\n",
    "best_model_file             = f\"{net_code_dir}/MultiLevelPyramidFeature_01/best_mae.pth\"\n",
    "\n",
    "####下記フォルダに、学習させたい画像と、Annotationデータをセットで配置します\n",
    "\n",
    "training_set_root_data_dir                          = training_data_root_dir2 + \"SetRootData/\"  #こちらの各ディレクトリにjsonと画像ファイルを配置する\n",
    "\n",
    "training_data_images_dir_before_trimming            = training_data_root_dir + \"images/ImageBeforeTrimming/\" #こちらに画像を配置する\n",
    "evaluation_data_images_dir_before_trimming          = training_data_root_dir + \"evaluation/ImageBeforeTrimming/\" #こちらに画像を配置する\n",
    "inference_input_path_before_trimming                = training_data_root_dir + \"inference/Input/image_before_trimming\" #こちらに画像を配置する\n",
    "\n",
    "training_data_training_label_dir                    = training_data_root_dir + \"labels_Training\" #こちらにjsonファイルを配置する\n",
    "training_data_evaluation_label_dir                  = training_data_root_dir + \"labels_Evaluation\" #こちらにjsonファイルを配置する\n",
    "\n",
    "####学習用パッチ画像を生成すると更新される\n",
    "training_data_images_dir                            = training_data_root_dir + \"images/\" #こちらにパッチ画像が生成される\n",
    "evaluation_data_images_dir                          = training_data_root_dir + \"evaluation/\" #こちらにパッチ画像が生成される\n",
    "\n",
    "training_data_label_dir_before_trimming             = training_data_root_dir + \"labels_Training/only_coordinate/ImageBeforeTrimming/\" #こちらにjsonを変換したtxtが生成される\n",
    "evaluation_data_label_dir_before_trimming           = training_data_root_dir + \"labels_Evaluation/only_coordinate/ImageBeforeTrimming/\" #こちらにjsonを変換したtxtが生成される\n",
    "\n",
    "training_data_points_dir                            = training_data_root_dir + \"labels_Training/only_coordinate/\" #こちらにtxtファイルをパッチサイズで切り出したファイルが生成される\n",
    "evaluation_data_points_dir                          = training_data_root_dir + \"labels_Evaluation/only_coordinate/\" #こちらにtxtファイルをパッチサイズで切り出したファイルが生成される\n",
    "\n",
    "training_list_path                                  = training_data_root_dir + \"training_list.txt\" #こちらに画像⇔txtの対応ファイルが生成される\n",
    "evaluating_list_path                                = training_data_root_dir + \"evaluation_list.txt\" #こちらに画像⇔txtの対応ファイルが生成される\n",
    "\n",
    "#推論用パッチ画像を生成すると更新される\n",
    "# inference_input_path                                = training_data_root_dir + \"inference/Input\"#こちらにパッチ画像が生成される\n",
    "inference_input_path                                = \"../data/inference\"\n",
    "# inference_output_path                               = training_data_root_dir + \"inference/Output\"#こちらに推論結果が生成される\n",
    "inference_output_path                               = \"./outputs/exp_v2\"\n",
    "\n",
    "vis_dir = f\"{inference_output_path}/visual_pyramid\"\n",
    "if not os.path.exists(vis_dir):\n",
    "    os.makedirs(vis_dir)\n",
    "\n",
    "checkpoints_dir = os.path.abspath(\"./outputs/exp_v2/models/\")\n",
    "if not os.path.exists(checkpoints_dir):\n",
    "    os.makedirs(checkpoints_dir)\n",
    "\n",
    "inference_integrated_image_file_path                = training_data_root_dir + \"inference/Output/Integrated\"#パッチ統合後画像が生成される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.insert(1, net_code_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# datasets.py\n",
    "###############\n",
    "\n",
    "class SHHA(Dataset):\n",
    "    def __init__(self, data_root, transform=None, train=False, patch=False, flip=False):\n",
    "        self.root_path = data_root\n",
    "\n",
    "        #############################\n",
    "        #   Global variables DANGER #\n",
    "        #############################\n",
    "        self.train_lists = training_list_path ## rice grainsの場合\n",
    "        self.eval_lists = evaluating_list_path ## rice grainsの場合\n",
    "\n",
    "        # there may exist multiple list files\n",
    "        # e.g. \n",
    "        # ```\n",
    "        # content/drive/MyDrive/research/tanashi2022/GrainCounting/AnalysisData_small/images/20230209_08_G_a_v05_h11.JPG,/content/drive/MyDrive/research/tanashi2022/GrainCounting/AnalysisData_small/labels_Training/only_coordinate/20230209_08_G_a_v05_h11.txt\n",
    "        # /content/drive/MyDrive/research/tanashi2022/GrainCounting/AnalysisData_small/images/20221227_07_R_v05_h01.JPG,/content/drive/MyDrive/research/tanashi2022/GrainCounting/AnalysisData_small/labels_Training/only_coordinate/20221227_07_R_v05_h01.txt\n",
    "        # if train:\n",
    "        #     self.img_list_file = [name.split(',') for name in open(self.train_lists).read().splitlines()]\n",
    "        # else:\n",
    "        #     self.img_list_file = [name.split(',') for name in open(self.eval_lists).read().splitlines()]\n",
    "\n",
    "        # self.img_list = self.img_list_file\n",
    "\n",
    "        if train:\n",
    "            self.img_list = train_img_list\n",
    "        else:\n",
    "            self.img_list = eval_img_list\n",
    "\n",
    "        # number of samples\n",
    "        self.nSamples = len(self.img_list)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.patch = patch\n",
    "        self.flip = flip\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert index <= len(self), 'index range error'\n",
    "\n",
    "\n",
    "        img_path = self.img_list[index][0]\n",
    "        gt_path = self.img_list[index][1]\n",
    "\n",
    "        # load image and ground truth\n",
    "        img, point, labels = load_data((img_path, gt_path), self.train)\n",
    "\n",
    "        # applu augumentation\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.train:\n",
    "            # data augmentation -> random scale\n",
    "            scale_range = [0.5, 1.3]\n",
    "            min_size = min(img.shape[1:])\n",
    "            scale = random.uniform(*scale_range)\n",
    "            # scale the image and points\n",
    "            if scale * min_size > trimming_size:\n",
    "                img = torch.nn.functional.upsample_bilinear(img.unsqueeze(0), scale_factor=scale).squeeze(0)\n",
    "                point *= scale\n",
    "        # random crop augumentaiton\n",
    "        if self.train and self.patch:\n",
    "            img, point,labels = random_crop(img, point, labels)\n",
    "\n",
    "            for i, _ in enumerate(point):  #03/21 debug\n",
    "                point[i] = torch.Tensor(point[i])\n",
    "                labels[i] = torch.Tensor(labels[i])\n",
    "\n",
    "        # random flipping\n",
    "        if random.random() > 0.5 and self.train and self.flip:\n",
    "            # random flip\n",
    "            img = torch.Tensor(img[:, :, :, ::-1].copy())\n",
    "\n",
    "            for i, _ in enumerate(point):\n",
    "                point[i][:, 0] = trimming_size - point[i][:, 0]\n",
    "\n",
    "        if not self.train:\n",
    "            point = [point]\n",
    "            labels = [labels]\n",
    "\n",
    "        img = torch.Tensor(img)\n",
    "        # pack up related infos\n",
    "        target = [{} for i in range(len(point))]\n",
    "\n",
    "        for i, _ in enumerate(point):  #03/21 debug\n",
    "            target[i]['point'] = torch.Tensor(point[i])\n",
    "\n",
    "            if len(labels[0]) > 1:\n",
    "                target[i]['labels'] = torch.Tensor(labels[i].flatten()).long()\n",
    "            else:\n",
    "                target[i]['labels'] = torch.Tensor(labels[i]).long()\n",
    "\n",
    "            image_id_1 = int(img_path.split('/')[-1].split('.')[0][5:7])\n",
    "            image_id_1 = torch.Tensor([image_id_1]).long()\n",
    "            #\n",
    "            image_id_2 = int(img_path.split('/')[-1].split('.')[0][5:7])\n",
    "            image_id_2 = torch.Tensor([image_id_2]).long()\n",
    "\n",
    "            target[i]['image_id_1'] = image_id_1\n",
    "            target[i]['image_id_2'] = image_id_2\n",
    "\n",
    "        return img, target\n",
    "\n",
    "\n",
    "def load_data(img_gt_path, train):\n",
    "    img_path, gt_path = img_gt_path\n",
    "\n",
    "    # load the images\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    if not img is None:\n",
    "        img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # load ground truth points\n",
    "    points = []\n",
    "    labels = []\n",
    "    #\n",
    "    pts = open(gt_path).read().splitlines()\n",
    "    for pt_0 in pts:\n",
    "        pt = eval(pt_0)\n",
    "        x = float(pt[0])#/2\n",
    "        y = float(pt[1])#/2\n",
    "        label = float(pt[2])\n",
    "        points.append([x, y])\n",
    "        labels.append([label])\n",
    "\n",
    "    return img, np.array(points), np.array(labels)\n",
    "\n",
    "\n",
    "# random crop augumentation\n",
    "def random_crop(img, den, labels, num_patch=1):\n",
    "    half_h = trimming_size\n",
    "    half_w = trimming_size\n",
    "    result_img = np.zeros([num_patch, img.shape[0], half_h, half_w])\n",
    "    result_den = []\n",
    "    result_label = []\n",
    "    # crop num_patch for each image\n",
    "    for i in range(num_patch):\n",
    "        start_h = random.randint(0, img.size(1) - half_h)\n",
    "        start_w = random.randint(0, img.size(2) - half_w)\n",
    "        end_h = start_h + half_h\n",
    "        end_w = start_w + half_w\n",
    "        # copy the cropped rect\n",
    "        result_img[i] = img[:, start_h:end_h, start_w:end_w]\n",
    "        # copy the cropped points\n",
    "        idx = (den[:, 0] >= start_w) & (den[:, 0] <= end_w) & (den[:, 1] >= start_h) & (den[:, 1] <= end_h)\n",
    "        # shift the corrdinates\n",
    "        record_den = den[idx]\n",
    "        record_label = labels[idx]\n",
    "        record_den[:, 0] -= start_w\n",
    "        record_den[:, 1] -= start_h\n",
    "\n",
    "        result_den.append(record_den)\n",
    "        result_label.append(record_label)\n",
    "\n",
    "    return result_img, result_den, result_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# load_data.py\n",
    "###############\n",
    "import torchvision.transforms as standard_transforms\n",
    "\n",
    "# DeNormalize used to get original images\n",
    "class DeNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return tensor\n",
    "\n",
    "def loading_data(data_root):\n",
    "    # the pre-proccssing transform\n",
    "    transform = standard_transforms.Compose([\n",
    "        standard_transforms.ToTensor(),\n",
    "        standard_transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    # create the training dataset\n",
    "    train_set = SHHA(data_root, train=True, transform=transform, patch=True, flip=True)\n",
    "    # create the validation dataset\n",
    "    val_set = SHHA(data_root, train=False, transform=transform)\n",
    "\n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcp2pnet.utils import DeNormalize, gen_random_scale_n\n",
    "from gcp2pnet.engine import train_one_epoch, evaluate_crowd_no_overlap\n",
    "\n",
    "import gcp2pnet.misc as utils\n",
    "\n",
    "# ############\n",
    "# # engine.py\n",
    "# ############\n",
    "# import math\n",
    "# import os\n",
    "# import sys\n",
    "# from typing import Iterable\n",
    "\n",
    "# import torch\n",
    "\n",
    "# sys.path.insert(0, os.path.join (program_files_root_dir,  \"CrowdCounting-P2PNet\") )\n",
    "# import util.misc as utils\n",
    "# from util.misc import NestedTensor\n",
    "# # this util is from previous P2PNet directory\n",
    "\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import torchvision.transforms as standard_transforms\n",
    "# import cv2\n",
    "\n",
    "# class DeNormalize(object):\n",
    "#     def __init__(self, mean, std):\n",
    "#         self.mean = mean\n",
    "#         self.std = std\n",
    "\n",
    "#     def __call__(self, tensor):\n",
    "#         for t, m, s in zip(tensor, self.mean, self.std):\n",
    "#             t.mul_(s).add_(m)\n",
    "#         return tensor\n",
    "# #\n",
    "# def gen_random_scale_n(img, rnd=3):\n",
    "#     np.random.seed(rnd)\n",
    "#     scale = torch.tensor(np.random.uniform(0.1, 1.91, (1,1, img.size()[2],img.size()[3]))).float()\n",
    "#     return torch.mul(img, scale)\n",
    "# ##\n",
    "# def vis(samples, targets, pred, vis_dir, epoch, predict_cnt, gt_cnt):\n",
    "#     '''\n",
    "#     samples -> tensor: [batch, 3, H, W]\n",
    "#     targets -> list of dict: [{'points':[], 'image_id': str}]\n",
    "#     pred -> list: [num_preds, 2]\n",
    "#     '''\n",
    "#     gts = [t['point'].tolist() for t in targets]\n",
    "\n",
    "#     pil_to_tensor = standard_transforms.ToTensor()\n",
    "\n",
    "#     restore_transform = standard_transforms.Compose([\n",
    "#         DeNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#         standard_transforms.ToPILImage()\n",
    "#     ])\n",
    "#     # draw one by one\n",
    "#     for idx in range(samples.shape[0]):\n",
    "#         sample = restore_transform(samples[idx])\n",
    "#         sample = pil_to_tensor(sample.convert('RGB')).numpy() * 255\n",
    "#         sample_gt = sample.transpose([1, 2, 0])[:, :, ::-1].astype(np.uint8).copy()\n",
    "#         sample_pred = sample.transpose([1, 2, 0])[:, :, ::-1].astype(np.uint8).copy()\n",
    "\n",
    "#         max_len = np.max(sample_gt.shape)\n",
    "\n",
    "#         size = 5\n",
    "#         # draw gt\n",
    "#         for t in gts[idx]:\n",
    "#             sample_gt = cv2.circle(sample_gt, (int(t[0]), int(t[1])), size, (0, 255, 0), -1)\n",
    "#         # draw predictions\n",
    "#         for p in pred[idx]:\n",
    "#             sample_pred = cv2.circle(sample_pred, (int(p[0]), int(p[1])), size, (0, 0, 255), -1)\n",
    "\n",
    "#         name_1 = targets[idx]['image_id_1']\n",
    "#         name_2 = targets[idx]['image_id_2']\n",
    "#         #################\n",
    "#         fig = plt.figure()\n",
    "#         ax1 = fig.add_subplot(1, 2, 1)\n",
    "#         ax1.imshow(sample_gt)\n",
    "#         ax1.get_xaxis().set_visible(False)\n",
    "#         ax1.get_yaxis().set_visible(False)\n",
    "#         ax2 = fig.add_subplot(1, 2, 2)\n",
    "#         ax2.imshow(sample_pred)\n",
    "#         ax2.get_xaxis().set_visible(False)\n",
    "#         ax2.get_yaxis().set_visible(False)\n",
    "#         fig.suptitle('manual count=%4.2f, inferred count=%4.2f'%(gt_cnt, predict_cnt), fontsize=10)\n",
    "#         plt.tight_layout(rect=[0, 0, 0.95, 0.95]) # maize tassels counting\n",
    "#         plt.savefig(os.path.join(vis_dir, '{}_{}_id_{}_ind_{}.jpg'.format(epoch, idx, int(name_1), int(name_2))), bbox_inches='tight', dpi = 300)\n",
    "#         plt.close()\n",
    "\n",
    "\n",
    "# # the training routine\n",
    "# def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n",
    "#                     data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "#                     device: torch.device, epoch: int, max_norm: float = 0):\n",
    "#     model.train()\n",
    "#     criterion.train()\n",
    "#     metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "#     metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "#     # iterate all training samples\n",
    "#     for samples, targets in data_loader:\n",
    "#         #print(\"samples {}\".format(samples.size()))\n",
    "#         #samples = gen_random_scale_n(samples)\n",
    "#         #\n",
    "#         samples = samples.to(device)\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "#         # forward\n",
    "#         outputs = model(samples)\n",
    "#         #\n",
    "# #        print(\"outputs\")\n",
    "# #        print(outputs)\n",
    "\n",
    "\n",
    "# #        print(\"targets\")\n",
    "# #        print(targets.shape)\n",
    "\n",
    "\n",
    "#         # calc the losses\n",
    "#         loss_dict = criterion(outputs, targets)\n",
    "#         weight_dict = criterion.weight_dict\n",
    "#         losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "#         # reduce all losses (get the mean values)\n",
    "#         loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "#         loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
    "#                                       for k, v in loss_dict_reduced.items()}\n",
    "#         loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
    "#                                     for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
    "#         losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
    "\n",
    "#         loss_value = losses_reduced_scaled.item()\n",
    "\n",
    "#         if not math.isfinite(loss_value):\n",
    "#             print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "#             print(loss_dict_reduced)\n",
    "#             sys.exit(1)\n",
    "#         # backward\n",
    "#         optimizer.zero_grad()\n",
    "#         losses.backward()\n",
    "#         if max_norm > 0:\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "#         optimizer.step()\n",
    "#         # update logger\n",
    "#         metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
    "#         metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "#     # gather the stats from all processes\n",
    "#     metric_logger.synchronize_between_processes()\n",
    "#     print(\"Averaged stats:\", metric_logger)\n",
    "#     return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # the inference routine\n",
    "# @torch.no_grad()\n",
    "# def evaluate_crowd_no_overlap(model, data_loader, device, epoch, threshold, vis_dir=None):\n",
    "#     model.eval()\n",
    "\n",
    "#     metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "#     metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
    "#     # run inference on all images to calc MAE\n",
    "#     maes = []\n",
    "#     mses = []\n",
    "#     for samples, targets in data_loader:\n",
    "\n",
    "#         gt_cnt = targets[0]['point'].shape[0]\n",
    "\n",
    "#         gt_cnt_0 = 0\n",
    "#         gt_cnt_1 = 0\n",
    "\n",
    "#         for label in targets[0]['labels']:\n",
    "#           if label == 1:\n",
    "#             gt_cnt_0 += 1\n",
    "#           elif label == 2:\n",
    "#             gt_cnt_1 += 1\n",
    "\n",
    "# ########################\n",
    "\n",
    "# #        samples = torch.Tensor(samples).unsqueeze(0)\n",
    "#         samples = samples.to(device)\n",
    "#         # run inference\n",
    "#         outputs = model(samples)\n",
    "\n",
    "#         outputs_points = outputs['pred_points'][0]\n",
    "\n",
    "#         outputs_scores = []\n",
    "#         for i in range(label_type_count - 1):\n",
    "#           outputs_scores.append(torch.nn.functional.softmax(outputs['pred_logits'], -1)[:, :, i + 1][0])\n",
    "\n",
    "\n",
    "\n",
    "#         # filter the predictions\n",
    "#         points_n_for_all_class = []\n",
    "#         scores_n_for_all_class = []\n",
    "\n",
    "#         count_0_for_all_points = []\n",
    "#         count_1_for_all_points = []\n",
    "\n",
    "\n",
    "#         for i in range(label_type_count - 1):\n",
    "\n",
    "#           points = outputs_points   [outputs_scores[i] > threshold].detach().cpu().numpy()#.tolist()\n",
    "#           scores = outputs_scores[i][outputs_scores[i] > threshold].detach().cpu().numpy()#.tolist()\n",
    "\n",
    "#           if points.shape[0]<10000 and points.shape[0] != 0:\n",
    "#               cutoff = 500/points.shape[0]\n",
    "#               if cutoff<20:\n",
    "#                   cutoff = 20\n",
    "#               components = nx.connected_components(\n",
    "#                   nx.from_edgelist(\n",
    "#                       (i, j) for i, js in enumerate(\n",
    "#                           spatial.KDTree(points).query_ball_point(points, cutoff)\n",
    "#                       )\n",
    "#                       for j in js\n",
    "#                   )\n",
    "#               )\n",
    "\n",
    "\n",
    "#               clusters = {j: i for i, js in enumerate(components) for j in js}\n",
    "\n",
    "\n",
    "#               # reorganize the points to the order of clusters\n",
    "#               points_reo = np.zeros(points.shape)\n",
    "#               scores_reo = np.zeros(scores.shape)\n",
    "\n",
    "#               i = 0\n",
    "#               for key in clusters.keys():\n",
    "#                   points_reo[i,:] = points[key,:]\n",
    "#                   scores_reo[i] = scores[key]\n",
    "#                   i+=1\n",
    "\n",
    "\n",
    "#               # points_n has the same order as clusters\n",
    "#               res = [clusters[key] for key in clusters.keys()]\n",
    "#               res_n = np.array(res).reshape(-1,1)\n",
    "\n",
    "\n",
    "#               points_n = []\n",
    "#               scores_n = []\n",
    "#               for i in np.unique(res_n):\n",
    "\n",
    "#                   tmp_points = points_reo[np.where(res_n[:,0] == i)]\n",
    "#                   tmp_scores = scores_reo[np.where(res_n[:,0] == i)]\n",
    "\n",
    "#                   points_n.append( [np.mean(tmp_points[:,0]), np.mean(tmp_points[:,1])])\n",
    "#                   scores_n.append( [np.mean(tmp_scores[:])])\n",
    "# #                    scores_n.append( [np.amax(tmp_scores[:])])\n",
    "\n",
    "#           else:\n",
    "#               points_n = points.tolist()\n",
    "#               scores_n = scores.tolist()\n",
    "\n",
    "#           points_n_for_all_class.append(points_n)\n",
    "#           scores_n_for_all_class.append(scores_n)\n",
    "\n",
    "\n",
    "#         #推論結果の点群を走査して、場所が重複するものがあれば、第三のカテゴリに変更する。(縦も横も7ピクセル以内かどうか)\n",
    "\n",
    "#         point_box = []\n",
    "#         score_box = []\n",
    "#         estimated_result = []\n",
    "\n",
    "\n",
    "#         for i, point_set in enumerate(points_n_for_all_class):\n",
    "#           for p in point_set:\n",
    "#               if i == 0:\n",
    "#                 point_box.append([p,0])\n",
    "#               if i == 1:\n",
    "#                 point_box.append([p,1])\n",
    "\n",
    "#         for i, score_set in enumerate(scores_n_for_all_class):\n",
    "#           for s in score_set:\n",
    "#               if i == 0:\n",
    "#                 score_box.append([s,0])\n",
    "#               if i == 1:\n",
    "#                 score_box.append([s,1])\n",
    "\n",
    "\n",
    "#         prox_distance = 25  #これより近い距離に別の点があれば、同じ点であるとみなして統合します。\n",
    "\n",
    "#         data_for_print = []\n",
    "\n",
    "#         count_0 = 0\n",
    "#         count_1 = 0\n",
    "\n",
    "#         #distance以内に、自分よりscoreが高い点が無ければ、valid_flagが１のまま残り、その点をカウントし、画像にプロットする。\n",
    "#         for i, test_point in enumerate(point_box):\n",
    "\n",
    "#           valid_flag = 1\n",
    "\n",
    "#           x_test_point = test_point[0][0]\n",
    "#           y_test_point = test_point[0][1]\n",
    "#           score_test   = score_box[i][0][0]\n",
    "#           class_test   = score_box[i][1]\n",
    "\n",
    "#           for j, compared_point in enumerate(point_box):\n",
    "#             x_compared_point = compared_point[0][0]\n",
    "#             y_compared_point = compared_point[0][1]\n",
    "#             score_compared   = score_box[j][0][0]\n",
    "#             dist = math.sqrt((x_test_point - x_compared_point) * (x_test_point - x_compared_point) + (y_test_point - y_compared_point) * (y_test_point - y_compared_point))\n",
    "\n",
    "#             if ((dist < prox_distance) and (score_test < score_compared)):\n",
    "#               valid_flag = 0\n",
    "\n",
    "#           if valid_flag == 1:\n",
    "#             if class_test == 0:\n",
    "#               count_0 = count_0 + 1\n",
    "#             else:\n",
    "#               count_1 = count_1 + 1\n",
    "\n",
    "\n",
    "# #########################\n",
    "#       # accumulate MAE, MSE\n",
    "#         mae = abs(count_0 - gt_cnt_0) + abs(count_1 - gt_cnt_1)\n",
    "#         mse = (count_0 - gt_cnt_0) * (count_0 - gt_cnt_0) + (count_1 - gt_cnt_1) * (count_1 - gt_cnt_1)\n",
    "\n",
    "# #        print(\"count_0:\" +  str(count_0) + \"  count_1:\" + str(count_1) + \"  gt_cnt_0:\" + str(gt_cnt_0) + \"  gt_cnt_1:\" + str(gt_cnt_1)+ \"  mae:\" + str(mae)+ \"  mse:\" + str(mse))\n",
    "\n",
    "#         maes.append(float(mae))\n",
    "#         mses.append(float(mse))\n",
    "#     # calc MAE, MSE\n",
    "#     mae = np.mean(maes)\n",
    "#     mse = np.sqrt(np.mean(mses))\n",
    "\n",
    "#     return mae, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcp2pnet.models.attention import SpatialAttention, ChannelwiseAttention\n",
    "\n",
    "# import argparse\n",
    "# import datetime\n",
    "# import random\n",
    "# import time\n",
    "# from pathlib import Path\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "# import os\n",
    "# from tensorboardX import SummaryWriter\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# ## attention module\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# #\n",
    "# class SpatialAttention(nn.Module):\n",
    "#     def __init__(self, in_channels, kernel_size=9):\n",
    "#         super(SpatialAttention, self).__init__()\n",
    "\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.in_channels = in_channels\n",
    "#         pad = (self.kernel_size-1)//2  # Padding on one side for stride 1\n",
    "\n",
    "#         self.grp1_conv1k = nn.Conv2d(self.in_channels, self.in_channels//2, (1, self.kernel_size), padding=(0, pad))\n",
    "#         self.grp1_bn1 = nn.BatchNorm2d(self.in_channels//2)\n",
    "#         self.grp1_convk1 = nn.Conv2d(self.in_channels//2, 1, (self.kernel_size, 1), padding=(pad, 0))\n",
    "#         self.grp1_bn2 = nn.BatchNorm2d(1)\n",
    "\n",
    "#         self.grp2_convk1 = nn.Conv2d(self.in_channels, self.in_channels//2, (self.kernel_size, 1), padding=(pad, 0))\n",
    "#         self.grp2_bn1 = nn.BatchNorm2d(self.in_channels//2)\n",
    "#         self.grp2_conv1k = nn.Conv2d(self.in_channels//2, 1, (1, self.kernel_size), padding=(0, pad))\n",
    "#         self.grp2_bn2 = nn.BatchNorm2d(1)\n",
    "\n",
    "#     def forward(self, input_):\n",
    "#         # Generate Group 1 Features\n",
    "#         grp1_feats = self.grp1_conv1k(input_)\n",
    "#         grp1_feats = F.relu(self.grp1_bn1(grp1_feats))\n",
    "#         grp1_feats = self.grp1_convk1(grp1_feats)\n",
    "#         grp1_feats = F.relu(self.grp1_bn2(grp1_feats))\n",
    "\n",
    "#         # Generate Group 2 features\n",
    "#         grp2_feats = self.grp2_convk1(input_)\n",
    "#         grp2_feats = F.relu(self.grp2_bn1(grp2_feats))\n",
    "#         grp2_feats = self.grp2_conv1k(grp2_feats)\n",
    "#         grp2_feats = F.relu(self.grp2_bn2(grp2_feats))\n",
    "\n",
    "#         added_feats = torch.sigmoid(torch.add(grp1_feats, grp2_feats))\n",
    "#         added_feats = added_feats.expand_as(input_).clone()\n",
    "\n",
    "#         return added_feats\n",
    "\n",
    "\n",
    "# class ChannelwiseAttention(nn.Module):\n",
    "#     def __init__(self, in_channels):\n",
    "#         super(ChannelwiseAttention, self).__init__()\n",
    "\n",
    "#         self.in_channels = in_channels\n",
    "\n",
    "#         self.linear_1 = nn.Linear(self.in_channels, self.in_channels//4)\n",
    "#         self.linear_2 = nn.Linear(self.in_channels//4, self.in_channels)\n",
    "\n",
    "#     def forward(self, input_):\n",
    "#         n_b, n_c, h, w = input_.size()\n",
    "\n",
    "#         feats = F.adaptive_avg_pool2d(input_, (1, 1)).view((n_b, n_c))\n",
    "#         feats = F.relu(self.linear_1(feats))\n",
    "#         feats = torch.sigmoid(self.linear_2(feats))\n",
    "\n",
    "#         # Activity regularizer\n",
    "#         ca_act_reg = torch.mean(feats)\n",
    "\n",
    "#         feats = feats.view((n_b, n_c, 1, 1))\n",
    "#         feats = feats.expand_as(input_).clone()\n",
    "\n",
    "#         return feats, ca_act_reg\n",
    "# ## note on the output: attention modules doesn't af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcp2pnet.models.pyrimid import (\n",
    "    vgg_conv1_2, vgg_conv2_2, vgg_conv3_3, vgg_conv4_3, vgg_conv5_3,\n",
    "    conv_1_2_hook, conv_2_2_hook, conv_3_3_hook, conv_4_3_hook, conv_5_3_hook,\n",
    "    CPFE, SODModel, \n",
    ")\n",
    "\n",
    "# ####                    pyramid feature attention. (here you adjust different feature levels and attentions)\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torchvision.models as models\n",
    "\n",
    "\n",
    "# vgg_conv1_2 = vgg_conv2_2 = vgg_conv3_3 = vgg_conv4_3 = vgg_conv5_3 = None\n",
    "\n",
    "\n",
    "# def conv_1_2_hook(module, input, output):\n",
    "#     global vgg_conv1_2\n",
    "#     vgg_conv1_2 = output\n",
    "#     return None\n",
    "\n",
    "# def conv_2_2_hook(module, input, output):\n",
    "#     global vgg_conv2_2\n",
    "#     vgg_conv2_2 = output\n",
    "#     return None\n",
    "\n",
    "# def conv_3_3_hook(module, input, output):\n",
    "#     global vgg_conv3_3\n",
    "#     vgg_conv3_3 = output\n",
    "#     return None\n",
    "\n",
    "# def conv_4_3_hook(module, input, output):\n",
    "#     global vgg_conv4_3\n",
    "#     vgg_conv4_3 = output\n",
    "#     return None\n",
    "\n",
    "# def conv_5_3_hook(module, input, output):\n",
    "#     global vgg_conv5_3\n",
    "#     vgg_conv5_3 = output\n",
    "#     return None\n",
    "\n",
    "# class CPFE(nn.Module):\n",
    "#     def __init__(self, feature_layer=None, out_channels=8):\n",
    "#         super(CPFE, self).__init__()\n",
    "\n",
    "#         self.dil_rates = [3, 5, 7]\n",
    "\n",
    "#         # Determine number of in_channels from VGG-16 feature layer\n",
    "#         if feature_layer == 'conv5_3':\n",
    "#             self.in_channels = 512\n",
    "#         elif feature_layer == 'conv4_3':\n",
    "#             self.in_channels = 512\n",
    "#         elif feature_layer == 'conv3_3':\n",
    "#             self.in_channels = 256\n",
    "\n",
    "#         # Define layers\n",
    "#         self.conv_1_1 = nn.Conv2d(in_channels=self.in_channels, out_channels=out_channels, kernel_size=1, bias=False)\n",
    "#         self.conv_dil_3 = nn.Conv2d(in_channels=self.in_channels, out_channels=out_channels, kernel_size=3,\n",
    "#                                     stride=1, dilation=self.dil_rates[0], padding=self.dil_rates[0], bias=False)\n",
    "#         self.conv_dil_5 = nn.Conv2d(in_channels=self.in_channels, out_channels=out_channels, kernel_size=3,\n",
    "#                                     stride=1, dilation=self.dil_rates[1], padding=self.dil_rates[1], bias=False)\n",
    "#         self.conv_dil_7 = nn.Conv2d(in_channels=self.in_channels, out_channels=out_channels, kernel_size=3,\n",
    "#                                     stride=1, dilation=self.dil_rates[2], padding=self.dil_rates[2], bias=False)\n",
    "\n",
    "#         self.bn = nn.BatchNorm2d(out_channels*4)\n",
    "\n",
    "#     def forward(self, input_):\n",
    "#         # Extract features\n",
    "#         conv_1_1_feats = self.conv_1_1(input_)\n",
    "#         conv_dil_3_feats = self.conv_dil_3(input_)\n",
    "#         conv_dil_5_feats = self.conv_dil_5(input_)\n",
    "#         conv_dil_7_feats = self.conv_dil_7(input_)\n",
    "\n",
    "#         # Aggregate features\n",
    "#         concat_feats = torch.cat((conv_1_1_feats, conv_dil_3_feats, conv_dil_5_feats, conv_dil_7_feats), dim=1)\n",
    "#         bn_feats = F.relu(self.bn(concat_feats))\n",
    "\n",
    "#         return bn_feats\n",
    "\n",
    "# class SODModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SODModel, self).__init__()\n",
    "\n",
    "#         # Load the [partial] VGG-16 model\n",
    "#         self.vgg16 = models.vgg16(pretrained=True).features\n",
    "\n",
    "#         # Extract and register intermediate features of VGG-16\n",
    "#         self.vgg16[3].register_forward_hook(conv_1_2_hook)\n",
    "#         self.vgg16[8].register_forward_hook(conv_2_2_hook)\n",
    "#         self.vgg16[15].register_forward_hook(conv_3_3_hook)\n",
    "#         self.vgg16[22].register_forward_hook(conv_4_3_hook)\n",
    "#         self.vgg16[29].register_forward_hook(conv_5_3_hook)\n",
    "\n",
    "#         # Initialize layers for high level (hl) feature (conv3_3, conv4_3, conv5_3) processing\n",
    "#         self.cpfe_conv3_3 = CPFE(feature_layer='conv3_3')\n",
    "#         self.cpfe_conv4_3 = CPFE(feature_layer='conv4_3')\n",
    "#         self.cpfe_conv5_3 = CPFE(feature_layer='conv5_3')\n",
    "#         #  channel attention, remove if not needed\n",
    "#         # self.cha_att = ChannelwiseAttention(in_channels=96)  # in_channels = 3 x (8 x 4)\n",
    "\n",
    "#         self.hl_conv1 = nn.Conv2d(96, 8, (3, 3), padding=1)\n",
    "#         self.hl_bn1 = nn.BatchNorm2d(8)\n",
    "\n",
    "#         # Initialize layers for low level (ll) feature (conv1_2 and conv2_2) processing\n",
    "#         #self.ll_conv_0 = nn.Conv2d(3, 8, (3, 3), padding=1)\n",
    "#         #self.ll_bn_0 = nn.BatchNorm2d(8)\n",
    "#         self.ll_conv_1 = nn.Conv2d(64, 8, (3, 3), padding=1)\n",
    "#         self.ll_bn_1 = nn.BatchNorm2d(8)\n",
    "#         self.ll_conv_2 = nn.Conv2d(128, 8, (3, 3), padding=1)\n",
    "#         self.ll_bn_2 = nn.BatchNorm2d(8)\n",
    "#         self.ll_conv_3 = nn.Conv2d(19, 8, (3, 3), padding=1)\n",
    "#         self.ll_bn_3 = nn.BatchNorm2d(8)\n",
    "\n",
    "#         self.spa_att = SpatialAttention(in_channels=8)\n",
    "\n",
    "#         # Initialize layers for fused features (ff) processing\n",
    "#         #self.ff_conv_1 = nn.Conv2d(16, 1, (3, 3), padding=1)\n",
    "\n",
    "#     def forward(self, input_):\n",
    "#         global vgg_conv1_2, vgg_conv2_2, vgg_conv3_3, vgg_conv4_3, vgg_conv5_3\n",
    "\n",
    "#         # Pass input_ through vgg16 to generate intermediate features\n",
    "#         self.vgg16(input_)\n",
    "#         # print(vgg_conv1_2.size())\n",
    "#         # print(vgg_conv2_2.size())\n",
    "#         # print(vgg_conv3_3.size())\n",
    "#         # print(vgg_conv4_3.size())\n",
    "#         # print(vgg_conv5_3.size())\n",
    "\n",
    "#         # Process high level features\n",
    "#         conv3_cpfe_feats = self.cpfe_conv3_3(vgg_conv3_3)\n",
    "#         conv4_cpfe_feats = self.cpfe_conv4_3(vgg_conv4_3)\n",
    "#         conv5_cpfe_feats = self.cpfe_conv5_3(vgg_conv5_3)\n",
    "\n",
    "#         conv4_cpfe_feats = F.interpolate(conv4_cpfe_feats, scale_factor=2, mode='bilinear', align_corners=True) # reduce spatial dimension by 2\n",
    "#         conv5_cpfe_feats = F.interpolate(conv5_cpfe_feats, scale_factor=4, mode='bilinear', align_corners=True)\n",
    "\n",
    "#         conv_345_feats = torch.cat((conv3_cpfe_feats, conv4_cpfe_feats, conv5_cpfe_feats), dim=1)\n",
    "\n",
    "#         # 11,03,2022, remove channel attention\n",
    "#         #conv_345_ca, ca_act_reg = self.cha_att(conv_345_feats)\n",
    "#         #conv_345_feats = torch.mul(conv_345_feats, conv_345_ca)\n",
    "\n",
    "#         conv_345_feats = self.hl_conv1(conv_345_feats)\n",
    "#         conv_345_feats = F.relu(self.hl_bn1(conv_345_feats))\n",
    "#         #conv_345_feats = F.interpolate(conv_345_feats, scale_factor=2, mode='bilinear', align_corners=True) # to increase the spatial resolution by 2 (is removed now)\n",
    "\n",
    "#         # Process low level features\n",
    "#         conv0_feats = input_ #add original image as low feature level input\n",
    "#         #conv0_feats = F.relu(self.ll_bn_0(conv0_feats))\n",
    "#         conv1_feats = self.ll_conv_1(vgg_conv1_2)\n",
    "#         conv1_feats = F.relu(self.ll_bn_1(conv1_feats))\n",
    "#         conv2_feats = self.ll_conv_2(vgg_conv2_2)\n",
    "#         conv2_feats = F.relu(self.ll_bn_2(conv2_feats))\n",
    "\n",
    "#         conv0_feats = F.interpolate(conv0_feats, scale_factor=0.25, mode='bilinear', align_corners=True)\n",
    "#         conv1_feats = F.interpolate(conv1_feats, scale_factor=0.25, mode='bilinear', align_corners=True)\n",
    "#         conv2_feats = F.interpolate(conv2_feats, scale_factor=0.5, mode='bilinear', align_corners=True)\n",
    "\n",
    "#         conv_12_feats = torch.cat((conv0_feats, conv1_feats, conv2_feats), dim=1)\n",
    "#         conv_12_feats = self.ll_conv_3(conv_12_feats)\n",
    "#         conv_12_feats = F.relu(self.ll_bn_3(conv_12_feats))\n",
    "\n",
    "#         conv_12_sa = self.spa_att(conv_12_feats)\n",
    "#         conv_12_feats = torch.mul(conv_12_feats, conv_12_sa)\n",
    "\n",
    "#         # Fused features\n",
    "#         fused_features_l = torch.sigmoid(conv_12_feats)\n",
    "#         fused_features_h = torch.sigmoid(conv_345_feats)\n",
    "#         #fused_feats_l = conv_12_feats\n",
    "#         #fused_features_lh = torch.add(conv_12_feats, conv_345_feats)\n",
    "#         #fused_features_lh = torch.sigmoid(fused_features_lh)\n",
    "#         #\n",
    "#         #fused_feats_h = conv_345_feats\n",
    "#         #fused_feats_h = torch.sigmoid(fused_feats_h)\n",
    "\n",
    "#         return fused_features_l, fused_features_h #, fused_feats_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\\nMostly copy-paste from DETR (https://github.com/facebookresearch/detr).\\n\\nimport torch\\nfrom scipy.optimize import linear_sum_assignment\\nfrom torch import nn\\n\\n\\nclass HungarianMatcher_Crowd(nn.Module):\\n    \"\"\"This class computes an assignment between the targets and the predictions of the network\\n\\n    For efficiency reasons, the targets don\\'t include the no_object. Because of this, in general,\\n    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\\n    while the others are un-matched (and thus treated as non-objects).\\n    \"\"\"\\n\\n    def __init__(self, cost_class: float = 1, cost_point: float = 1):\\n        \"\"\"Creates the matcher\\n\\n        Params:\\n            cost_class: This is the relative weight of the foreground object\\n            cost_point: This is the relative weight of the L1 error of the points coordinates in the matching cost\\n        \"\"\"\\n        super().__init__()\\n        self.cost_class = cost_class\\n        self.cost_point = cost_point\\n        assert cost_class != 0 or cost_point != 0, \"all costs cant be 0\"\\n\\n    @torch.no_grad()\\n    def forward(self, outputs, targets):\\n        \"\"\" Performs the matching\\n\\n        Params:\\n            outputs: This is a dict that contains at least these entries:\\n                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\\n                 \"points\": Tensor of dim [batch_size, num_queries, 2] with the predicted point coordinates\\n\\n            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\\n                 \"labels\": Tensor of dim [num_target_points] (where num_target_points is the number of ground-truth\\n                           objects in the target) containing the class labels\\n                 \"points\": Tensor of dim [num_target_points, 2] containing the target point coordinates\\n\\n        Returns:\\n            A list of size batch_size, containing tuples of (index_i, index_j) where:\\n                - index_i is the indices of the selected predictions (in order)\\n                - index_j is the indices of the corresponding selected targets (in order)\\n            For each batch element, it holds:\\n                len(index_i) = len(index_j) = min(num_queries, num_target_points)\\n        \"\"\"\\n        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\\n\\n        # We flatten to compute the cost matrices in a batch\\n        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\\n        out_points = outputs[\"pred_points\"].flatten(0, 1)  # [batch_size * num_queries, 2]\\n\\n        # Also concat the target labels and points\\n        # tgt_ids = torch.cat([v[\"labels\"] for v in targets])\\n        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\\n        tgt_points = torch.cat([v[\"point\"] for v in targets])\\n\\n\\n#03/21 debug        print(\"target:\")\\n#03/21 debug        print(targets)\\n#03/21 debug        print(\"target_ids:\")\\n#03/21 debug        print(tgt_ids)\\n#03/21 debug        print(\"target_points:\")\\n#03/21 debug        print(tgt_points)\\n\\n#03/21 debug        print(self.cost_point)\\n#03/21 debug        print(self.cost_class)\\n\\n\\n        # Compute the classification cost. Contrary to the loss, we don\\'t use the NLL,\\n        # but approximate it in 1 - proba[target class].\\n        # The 1 is a constant that doesn\\'t change the matching, it can be ommitted.\\n        cost_class = -out_prob[:, tgt_ids]\\n\\n        # Compute the L2 cost between point\\n#03/21 debug        print(\"out_points:\")\\n#03/21 debug        print(out_points)\\n#03/21 debug        print(\"tgt_points:\")\\n#03/21 debug        print(tgt_points)\\n        cost_point = torch.cdist(out_points, tgt_points, p=2)\\n\\n        # Compute the giou cost between point\\n\\n        # Final cost matrix\\n#03/21 debug        print(\"cost_point:\")\\n#03/21 debug        print(cost_point.shape)\\n#03/21 debug        print(\"cost_class:\")\\n#03/21 debug        print(cost_class.shape)\\n#03/21 debug        print(cost_class.squeeze().shape)\\n#03/21 debug        print(\"self.cost_point:\")\\n#03/21 debug        print(self.cost_point)\\n#03/21 debug        print(\"self.cost_class:\")\\n#03/21 debug        print(self.cost_class)\\n\\n\\n#3/25 debug        C = self.cost_point * cost_point + self.cost_class * cost_class.squeeze()\\n        if cost_class.dim() == 3:\\n          cost_class = torch.squeeze(cost_class, 2)\\n\\n        C = self.cost_point * cost_point + self.cost_class * cost_class\\n        C = C.view(bs, num_queries, -1).cpu()\\n\\n        sizes = [len(v[\"point\"]) for v in targets]\\n\\n\\n#03/21 debug        print(\"sizes\")\\n#03/21 debug        print(sizes[0])\\n#03/21 debug        print(np.array(sizes).shape)\\n\\n\\n\\n#03/21 debug        print(\"C\")\\n#03/21 debug        print(np.array(C).shape)\\n\\n        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\\n\\n#03/21 debug                indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\\n        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\\n\\n\\ndef build_matcher_crowd(args):\\n    return HungarianMatcher_Crowd(cost_class=args.set_cost_class, cost_point=args.set_cost_point)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gcp2pnet.models.matcher import HungarianMatcher_Crowd\n",
    "'''\n",
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "Mostly copy-paste from DETR (https://github.com/facebookresearch/detr).\n",
    "\n",
    "import torch\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class HungarianMatcher_Crowd(nn.Module):\n",
    "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
    "\n",
    "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
    "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
    "    while the others are un-matched (and thus treated as non-objects).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cost_class: float = 1, cost_point: float = 1):\n",
    "        \"\"\"Creates the matcher\n",
    "\n",
    "        Params:\n",
    "            cost_class: This is the relative weight of the foreground object\n",
    "            cost_point: This is the relative weight of the L1 error of the points coordinates in the matching cost\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_point = cost_point\n",
    "        assert cost_class != 0 or cost_point != 0, \"all costs cant be 0\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" Performs the matching\n",
    "\n",
    "        Params:\n",
    "            outputs: This is a dict that contains at least these entries:\n",
    "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
    "                 \"points\": Tensor of dim [batch_size, num_queries, 2] with the predicted point coordinates\n",
    "\n",
    "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
    "                 \"labels\": Tensor of dim [num_target_points] (where num_target_points is the number of ground-truth\n",
    "                           objects in the target) containing the class labels\n",
    "                 \"points\": Tensor of dim [num_target_points, 2] containing the target point coordinates\n",
    "\n",
    "        Returns:\n",
    "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "                - index_i is the indices of the selected predictions (in order)\n",
    "                - index_j is the indices of the corresponding selected targets (in order)\n",
    "            For each batch element, it holds:\n",
    "                len(index_i) = len(index_j) = min(num_queries, num_target_points)\n",
    "        \"\"\"\n",
    "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "\n",
    "        # We flatten to compute the cost matrices in a batch\n",
    "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "        out_points = outputs[\"pred_points\"].flatten(0, 1)  # [batch_size * num_queries, 2]\n",
    "\n",
    "        # Also concat the target labels and points\n",
    "        # tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
    "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
    "        tgt_points = torch.cat([v[\"point\"] for v in targets])\n",
    "\n",
    "\n",
    "#03/21 debug        print(\"target:\")\n",
    "#03/21 debug        print(targets)\n",
    "#03/21 debug        print(\"target_ids:\")\n",
    "#03/21 debug        print(tgt_ids)\n",
    "#03/21 debug        print(\"target_points:\")\n",
    "#03/21 debug        print(tgt_points)\n",
    "\n",
    "#03/21 debug        print(self.cost_point)\n",
    "#03/21 debug        print(self.cost_class)\n",
    "\n",
    "\n",
    "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "        # but approximate it in 1 - proba[target class].\n",
    "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "        cost_class = -out_prob[:, tgt_ids]\n",
    "\n",
    "        # Compute the L2 cost between point\n",
    "#03/21 debug        print(\"out_points:\")\n",
    "#03/21 debug        print(out_points)\n",
    "#03/21 debug        print(\"tgt_points:\")\n",
    "#03/21 debug        print(tgt_points)\n",
    "        cost_point = torch.cdist(out_points, tgt_points, p=2)\n",
    "\n",
    "        # Compute the giou cost between point\n",
    "\n",
    "        # Final cost matrix\n",
    "#03/21 debug        print(\"cost_point:\")\n",
    "#03/21 debug        print(cost_point.shape)\n",
    "#03/21 debug        print(\"cost_class:\")\n",
    "#03/21 debug        print(cost_class.shape)\n",
    "#03/21 debug        print(cost_class.squeeze().shape)\n",
    "#03/21 debug        print(\"self.cost_point:\")\n",
    "#03/21 debug        print(self.cost_point)\n",
    "#03/21 debug        print(\"self.cost_class:\")\n",
    "#03/21 debug        print(self.cost_class)\n",
    "\n",
    "\n",
    "#3/25 debug        C = self.cost_point * cost_point + self.cost_class * cost_class.squeeze()\n",
    "        if cost_class.dim() == 3:\n",
    "          cost_class = torch.squeeze(cost_class, 2)\n",
    "\n",
    "        C = self.cost_point * cost_point + self.cost_class * cost_class\n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "        sizes = [len(v[\"point\"]) for v in targets]\n",
    "\n",
    "\n",
    "#03/21 debug        print(\"sizes\")\n",
    "#03/21 debug        print(sizes[0])\n",
    "#03/21 debug        print(np.array(sizes).shape)\n",
    "\n",
    "\n",
    "\n",
    "#03/21 debug        print(\"C\")\n",
    "#03/21 debug        print(np.array(C).shape)\n",
    "\n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "\n",
    "#03/21 debug                indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
    "\n",
    "\n",
    "def build_matcher_crowd(args):\n",
    "    return HungarianMatcher_Crowd(cost_class=args.set_cost_class, cost_point=args.set_cost_point)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n############\\n# p2pnet.py\\n############\\n\\n# build model p2pNet.py\\nimport torch\\nimport torch.nn.functional as F\\nfrom torch import nn\\n\\nfrom util.misc import (NestedTensor, nested_tensor_from_tensor_list,\\n                       accuracy, get_world_size, interpolate,\\n                       is_dist_avail_and_initialized)\\n\\n# from models.backbone import build_backbone\\n# from models.matcher import build_matcher_crowd\\n\\nimport numpy as np\\nimport time\\n\\n# the network frmawork of the regression branch\\nclass RegressionModel(nn.Module):\\n    def __init__(self, num_features_in, num_anchor_points=4, feature_size=32):\\n        super(RegressionModel, self).__init__()\\n\\n        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\\n        self.act1 = nn.ReLU()\\n\\n        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\\n        self.act2 = nn.ReLU()\\n\\n        self.output = nn.Conv2d(feature_size, num_anchor_points * 2, kernel_size=3, padding=1) # one point has two coordinates\\n    # sub-branch forward\\n    def forward(self, x):\\n        out = self.conv1(x)\\n        out = self.act1(out)\\n\\n        out = self.conv2(out)\\n        out = self.act2(out)\\n\\n        out = self.output(out)\\n\\n        out = out.permute(0, 2, 3, 1)\\n\\n        return out.contiguous().view(out.shape[0], -1, 2)\\n\\n# the network frmawork of the classification branch\\nclass ClassificationModel(nn.Module):\\n    def __init__(self, num_features_in, num_anchor_points=4, num_classes=80, prior=0.01, feature_size=32):\\n        super(ClassificationModel, self).__init__()\\n\\n        self.num_classes = num_classes\\n        self.num_anchor_points = num_anchor_points\\n\\n        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\\n        self.act1 = nn.ReLU()\\n\\n        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\\n        self.act2 = nn.ReLU()\\n\\n        self.output = nn.Conv2d(feature_size, num_anchor_points * num_classes, kernel_size=3, padding=1) # one classes, only positives\\n        self.output_act = nn.Sigmoid()\\n    # sub-branch forward\\n    def forward(self, x):\\n        out = self.conv1(x)\\n        out = self.act1(out)\\n\\n        out = self.conv2(out)\\n        out = self.act2(out)\\n\\n        out = self.output(out)\\n\\n        out1 = out.permute(0, 2, 3, 1)\\n\\n        batch_size, width, height, _ = out1.shape\\n\\n        out2 = out1.view(batch_size, width, height, self.num_anchor_points, self.num_classes)\\n\\n        return out2.contiguous().view(x.shape[0], -1, self.num_classes)\\n\\n# generate the reference points in grid layout\\ndef generate_anchor_points(stride=8, row=3, line=3):\\n    row_step = stride / row\\n    line_step = stride / line\\n\\n    shift_x = (np.arange(1, line + 1) - 0.5) * line_step - stride / 2\\n    shift_y = (np.arange(1, row + 1) - 0.5) * row_step - stride / 2\\n\\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\\n\\n    anchor_points = np.vstack((\\n        shift_x.ravel(), shift_y.ravel()\\n    )).transpose()\\n\\n    return anchor_points\\n# shift the meta-anchor to get an acnhor points\\ndef shift(shape, stride, anchor_points):\\n    shift_x = (np.arange(0, shape[1]) + 0.5)* stride\\n    shift_y = (np.arange(0, shape[0]) + 0.5)* stride\\n\\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\\n\\n    shifts = np.vstack((\\n        shift_x.ravel(), shift_y.ravel()\\n    )).transpose()\\n\\n    A = anchor_points.shape[0]\\n    K = shifts.shape[0]\\n    all_anchor_points = (anchor_points.reshape((1, A, 2)) + shifts.reshape((1, K, 2)).transpose((1, 0, 2)))\\n    all_anchor_points = all_anchor_points.reshape((K * A, 2))\\n\\n    return all_anchor_points\\n\\n# this class generate all reference points on all pyramid levels\\nclass AnchorPoints(nn.Module):\\n    def __init__(self, pyramid_levels=None, strides=None, row=3, line=3):\\n        super(AnchorPoints, self).__init__()\\n\\n        if pyramid_levels is None:\\n            self.pyramid_levels = [3, 4, 5, 6, 7]\\n        else:\\n            self.pyramid_levels = pyramid_levels\\n\\n        if strides is None:\\n            self.strides = [2 ** x for x in self.pyramid_levels]\\n\\n        self.row = row\\n        self.line = line\\n\\n    def forward(self, image):\\n        image_shape = image.shape[2:]\\n        image_shape = np.array(image_shape)\\n        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels] # calcualtes the output size of the model (image of 128*128 to feature map of 16*16)\\n\\n        all_anchor_points = np.zeros((0, 2)).astype(np.float32)\\n        # get reference points for each level\\n        for idx, p in enumerate(self.pyramid_levels):\\n            anchor_points = generate_anchor_points(2**p, row=self.row, line=self.line)\\n            shifted_anchor_points = shift(image_shapes[idx], self.strides[idx], anchor_points)\\n            all_anchor_points = np.append(all_anchor_points, shifted_anchor_points, axis=0)\\n\\n        all_anchor_points = np.expand_dims(all_anchor_points, axis=0)\\n        # send reference points to device\\n        if torch.cuda.is_available():\\n            return torch.from_numpy(all_anchor_points.astype(np.float32)).cuda()\\n        else:\\n            return torch.from_numpy(all_anchor_points.astype(np.float32))\\n##\\n# the defenition of the P2PNet model\\nclass P2PNet(nn.Module):\\n    def __init__(self, row=2, line=2):\\n        super().__init__()\\n        self.num_classes = label_type_count\\n#        self.num_classes = label_type_count + 1\\n        # the number of all anchor points\\n        num_anchor_points = row * line\\n\\n        self.regression = RegressionModel(num_features_in=8, num_anchor_points=num_anchor_points)\\n        self.classification = ClassificationModel(num_features_in=8,                                             num_classes=self.num_classes,                                             num_anchor_points=num_anchor_points)\\n\\n        self.anchor_points = AnchorPoints(pyramid_levels=[2,], row=row, line=line) # remember to change pyramid level when you change feature input\\n\\n        #self.fpn = Decoder_stg3(128, 256, 512, 512)\\n        self.fpn = SODModel()\\n\\n    def forward(self, samples: NestedTensor):\\n        # get the backbone features\\n        # forward the feature pyramid\\n        features_fpn_l, features_fpn_h = self.fpn(samples) # output = bach_size, channel, Height, Weight\\n\\n        batch_size = features_fpn_l.size()[0]\\n        #print(\"features_fpn_1 size\")\\n        #print(features_fpn_1.size())\\n        #print(features_fpn[1].size())\\n        #print(features_fpn[2].size())\\n        #  put low level features for points regression\\n        regression = self.regression(features_fpn_l) * 100 # 8x\\n        #  put high level features for classification\\n        classification = self.classification(features_fpn_h)\\n        # generate reference points\\n        anchor_points = self.anchor_points(samples).repeat(batch_size, 1, 1)\\n        # decode the points as prediction\\n        #print(\"regression {}\".format(regression.size()))\\n        #print(\"anchor_points {}\".format(anchor_points.size()))\\n        output_coord = regression + anchor_points\\n        output_class = classification\\n        out = {\\'pred_logits\\': output_class, \\'pred_points\\': output_coord}\\n\\n        return out\\n\\nclass SetCriterion_Crowd(nn.Module):\\n\\n    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\\n        \"\"\" Create the criterion.\\n        Parameters:\\n            num_classes: number of object categories, omitting the special no-object category\\n            matcher: module able to compute a matching between targets and proposals\\n            weight_dict: dict containing as key the names of the losses and as values their relative weight.\\n            eos_coef: relative classification weight applied to the no-object category\\n            losses: list of all the losses to be applied. See get_loss for list of available losses.\\n        \"\"\"\\n        super().__init__()\\n        self.num_classes = num_classes\\n        self.matcher = matcher\\n        self.weight_dict = weight_dict\\n        self.eos_coef = eos_coef\\n        self.losses = losses\\n#        empty_weight = torch.ones(self.num_classes + 1)  03/19バグ修正\\n        empty_weight = torch.ones(self.num_classes)\\n        empty_weight[0] = self.eos_coef\\n        self.register_buffer(\\'empty_weight\\', empty_weight)\\n\\n#03/21 debug        print(\"weight_vector:\")\\n#        print(empty_weight.())\\n\\n    def loss_labels(self, outputs, targets, indices, num_points):\\n        \"\"\"Classification loss (NLL)\\n        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\\n        \"\"\"\\n        assert \\'pred_logits\\' in outputs\\n        src_logits = outputs[\\'pred_logits\\']\\n\\n        idx = self._get_src_permutation_idx(indices)\\n        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\\n        target_classes = torch.full(src_logits.shape[:2], 0,\\n                                    dtype=torch.int64, device=src_logits.device)\\n        target_classes[idx] = target_classes_o.squeeze()\\n\\n        torch.set_printoptions(edgeitems=1000)\\n#03/21 debug        print(\"logits\")\\n#03/21 debug        print(src_logits.transpose(1, 2).size())\\n#03/21 debug        print(\"target_class:\")\\n#03/21 debug        print(target_classes.size())\\n#03/21 debug        print(\"weight:\")\\n#03/21 debug        print(self.empty_weight.size())\\n\\n        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\\n        losses = {\\'loss_ce\\': loss_ce}\\n\\n        return losses\\n\\n    def loss_points(self, outputs, targets, indices, num_points):\\n\\n        assert \\'pred_points\\' in outputs\\n        idx = self._get_src_permutation_idx(indices)\\n        src_points = outputs[\\'pred_points\\'][idx]\\n        target_points = torch.cat([t[\\'point\\'][i] for t, (_, i) in zip(targets, indices)], dim=0)\\n        loss_bbox = F.mse_loss(src_points, target_points, reduction=\\'none\\')\\n\\n        losses = {}\\n        losses[\\'loss_points\\'] = loss_bbox.sum() / num_points\\n\\n        return losses\\n\\n    def _get_src_permutation_idx(self, indices):\\n        # permute predictions following indices\\n        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\\n        src_idx = torch.cat([src for (src, _) in indices])\\n        return batch_idx, src_idx\\n\\n    def _get_tgt_permutation_idx(self, indices):\\n        # permute targets following indices\\n        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\\n        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\\n        return batch_idx, tgt_idx\\n\\n    def get_loss(self, loss, outputs, targets, indices, num_points, **kwargs):\\n        loss_map = {\\n            \\'labels\\': self.loss_labels,\\n            \\'points\\': self.loss_points,\\n        }\\n#03/21 debug        print(\"loss_map {}\".format(loss_map))\\n        assert loss in loss_map, f\\'do you really want to compute {loss} loss?\\'\\n        return loss_map[loss](outputs, targets, indices, num_points, **kwargs)\\n\\n    def forward(self, outputs, targets):\\n        \"\"\" This performs the loss computation.\\n        Parameters:\\n             outputs: dict of tensors, see the output specification of the model for the format\\n             targets: list of dicts, such that len(targets) == batch_size.\\n                      The expected keys in each dict depends on the losses applied, see each loss\\' doc\\n        \"\"\"\\n        output1 = {\\'pred_logits\\': outputs[\\'pred_logits\\'], \\'pred_points\\': outputs[\\'pred_points\\']}\\n        indices1 = self.matcher(output1, targets)\\n\\n        num_points = sum(len(t[\"labels\"]) for t in targets)\\n\\n        num_points = torch.as_tensor([num_points], dtype=torch.float, device=next(iter(output1.values())).device)\\n        if is_dist_avail_and_initialized():\\n            torch.distributed.all_reduce(num_points)\\n        #\\n        num_boxes = torch.clamp(num_points / get_world_size(), min=1).item()\\n\\n        losses = {}\\n        for loss in self.losses:\\n            losses.update(self.get_loss(loss, output1, targets, indices1, num_boxes))\\n\\n        return losses\\n\\n# create the P2PNet model\\ndef build(args, training):\\n    # treats persons as a single class\\n    num_classes = label_type_count\\n\\n    model = P2PNet(args.row, args.line)\\n    if not training:\\n        return model\\n\\n    weight_dict = {\\'loss_ce\\': label_type_count, \\'loss_points\\': args.point_loss_coef}\\n#    weight_dict = {\\'loss_ce\\': 1, \\'loss_points\\': args.point_loss_coef}  2023/03/19 バグ取り\\n    losses = [\\'labels\\', \\'points\\']\\n    matcher = build_matcher_crowd(args)\\n    criterion = SetCriterion_Crowd(num_classes,                                 matcher=matcher, weight_dict=weight_dict,                                 eos_coef=args.eos_coef, losses=losses)\\n\\n#03/21 debug    print(\"criterion:\")\\n#03/21 debug    print(criterion)\\n    return model, criterion\\n###\\n# create the P2PNet model for execution\\ndef build_eval(args):\\n    # treats persons as a single class\\n    num_classes = label_type_count\\n\\n    model = P2PNet(args.row, args.line)\\n    return model\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gcp2pnet.models.p2pnet import (\n",
    "    RegressionModel, ClassificationModel, \n",
    "    generate_anchor_points, shift, \n",
    "    AnchorPoints, P2PNet, SetCriterion_Crowd, \n",
    "    build, build_eval,\n",
    ")\n",
    "\n",
    "'''\n",
    "############\n",
    "# p2pnet.py\n",
    "############\n",
    "\n",
    "# build model p2pNet.py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from util.misc import (NestedTensor, nested_tensor_from_tensor_list,\n",
    "                       accuracy, get_world_size, interpolate,\n",
    "                       is_dist_avail_and_initialized)\n",
    "\n",
    "# from models.backbone import build_backbone\n",
    "# from models.matcher import build_matcher_crowd\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# the network frmawork of the regression branch\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, num_features_in, num_anchor_points=4, feature_size=32):\n",
    "        super(RegressionModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchor_points * 2, kernel_size=3, padding=1) # one point has two coordinates\n",
    "    # sub-branch forward\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        return out.contiguous().view(out.shape[0], -1, 2)\n",
    "\n",
    "# the network frmawork of the classification branch\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, num_features_in, num_anchor_points=4, num_classes=80, prior=0.01, feature_size=32):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchor_points = num_anchor_points\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Conv2d(feature_size, num_anchor_points * num_classes, kernel_size=3, padding=1) # one classes, only positives\n",
    "        self.output_act = nn.Sigmoid()\n",
    "    # sub-branch forward\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        out = self.output(out)\n",
    "\n",
    "        out1 = out.permute(0, 2, 3, 1)\n",
    "\n",
    "        batch_size, width, height, _ = out1.shape\n",
    "\n",
    "        out2 = out1.view(batch_size, width, height, self.num_anchor_points, self.num_classes)\n",
    "\n",
    "        return out2.contiguous().view(x.shape[0], -1, self.num_classes)\n",
    "\n",
    "# generate the reference points in grid layout\n",
    "def generate_anchor_points(stride=8, row=3, line=3):\n",
    "    row_step = stride / row\n",
    "    line_step = stride / line\n",
    "\n",
    "    shift_x = (np.arange(1, line + 1) - 0.5) * line_step - stride / 2\n",
    "    shift_y = (np.arange(1, row + 1) - 0.5) * row_step - stride / 2\n",
    "\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "\n",
    "    anchor_points = np.vstack((\n",
    "        shift_x.ravel(), shift_y.ravel()\n",
    "    )).transpose()\n",
    "\n",
    "    return anchor_points\n",
    "# shift the meta-anchor to get an acnhor points\n",
    "def shift(shape, stride, anchor_points):\n",
    "    shift_x = (np.arange(0, shape[1]) + 0.5)* stride\n",
    "    shift_y = (np.arange(0, shape[0]) + 0.5)* stride\n",
    "\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "\n",
    "    shifts = np.vstack((\n",
    "        shift_x.ravel(), shift_y.ravel()\n",
    "    )).transpose()\n",
    "\n",
    "    A = anchor_points.shape[0]\n",
    "    K = shifts.shape[0]\n",
    "    all_anchor_points = (anchor_points.reshape((1, A, 2)) + shifts.reshape((1, K, 2)).transpose((1, 0, 2)))\n",
    "    all_anchor_points = all_anchor_points.reshape((K * A, 2))\n",
    "\n",
    "    return all_anchor_points\n",
    "\n",
    "# this class generate all reference points on all pyramid levels\n",
    "class AnchorPoints(nn.Module):\n",
    "    def __init__(self, pyramid_levels=None, strides=None, row=3, line=3):\n",
    "        super(AnchorPoints, self).__init__()\n",
    "\n",
    "        if pyramid_levels is None:\n",
    "            self.pyramid_levels = [3, 4, 5, 6, 7]\n",
    "        else:\n",
    "            self.pyramid_levels = pyramid_levels\n",
    "\n",
    "        if strides is None:\n",
    "            self.strides = [2 ** x for x in self.pyramid_levels]\n",
    "\n",
    "        self.row = row\n",
    "        self.line = line\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_shape = image.shape[2:]\n",
    "        image_shape = np.array(image_shape)\n",
    "        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels] # calcualtes the output size of the model (image of 128*128 to feature map of 16*16)\n",
    "\n",
    "        all_anchor_points = np.zeros((0, 2)).astype(np.float32)\n",
    "        # get reference points for each level\n",
    "        for idx, p in enumerate(self.pyramid_levels):\n",
    "            anchor_points = generate_anchor_points(2**p, row=self.row, line=self.line)\n",
    "            shifted_anchor_points = shift(image_shapes[idx], self.strides[idx], anchor_points)\n",
    "            all_anchor_points = np.append(all_anchor_points, shifted_anchor_points, axis=0)\n",
    "\n",
    "        all_anchor_points = np.expand_dims(all_anchor_points, axis=0)\n",
    "        # send reference points to device\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.from_numpy(all_anchor_points.astype(np.float32)).cuda()\n",
    "        else:\n",
    "            return torch.from_numpy(all_anchor_points.astype(np.float32))\n",
    "##\n",
    "# the defenition of the P2PNet model\n",
    "class P2PNet(nn.Module):\n",
    "    def __init__(self, row=2, line=2):\n",
    "        super().__init__()\n",
    "        self.num_classes = label_type_count\n",
    "#        self.num_classes = label_type_count + 1\n",
    "        # the number of all anchor points\n",
    "        num_anchor_points = row * line\n",
    "\n",
    "        self.regression = RegressionModel(num_features_in=8, num_anchor_points=num_anchor_points)\n",
    "        self.classification = ClassificationModel(num_features_in=8, \\\n",
    "                                            num_classes=self.num_classes, \\\n",
    "                                            num_anchor_points=num_anchor_points)\n",
    "\n",
    "        self.anchor_points = AnchorPoints(pyramid_levels=[2,], row=row, line=line) # remember to change pyramid level when you change feature input\n",
    "\n",
    "        #self.fpn = Decoder_stg3(128, 256, 512, 512)\n",
    "        self.fpn = SODModel()\n",
    "\n",
    "    def forward(self, samples: NestedTensor):\n",
    "        # get the backbone features\n",
    "        # forward the feature pyramid\n",
    "        features_fpn_l, features_fpn_h = self.fpn(samples) # output = bach_size, channel, Height, Weight\n",
    "\n",
    "        batch_size = features_fpn_l.size()[0]\n",
    "        #print(\"features_fpn_1 size\")\n",
    "        #print(features_fpn_1.size())\n",
    "        #print(features_fpn[1].size())\n",
    "        #print(features_fpn[2].size())\n",
    "        #  put low level features for points regression\n",
    "        regression = self.regression(features_fpn_l) * 100 # 8x\n",
    "        #  put high level features for classification\n",
    "        classification = self.classification(features_fpn_h)\n",
    "        # generate reference points\n",
    "        anchor_points = self.anchor_points(samples).repeat(batch_size, 1, 1)\n",
    "        # decode the points as prediction\n",
    "        #print(\"regression {}\".format(regression.size()))\n",
    "        #print(\"anchor_points {}\".format(anchor_points.size()))\n",
    "        output_coord = regression + anchor_points\n",
    "        output_class = classification\n",
    "        out = {'pred_logits': output_class, 'pred_points': output_coord}\n",
    "\n",
    "        return out\n",
    "\n",
    "class SetCriterion_Crowd(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n",
    "        \"\"\" Create the criterion.\n",
    "        Parameters:\n",
    "            num_classes: number of object categories, omitting the special no-object category\n",
    "            matcher: module able to compute a matching between targets and proposals\n",
    "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
    "            eos_coef: relative classification weight applied to the no-object category\n",
    "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.eos_coef = eos_coef\n",
    "        self.losses = losses\n",
    "#        empty_weight = torch.ones(self.num_classes + 1)  03/19バグ修正\n",
    "        empty_weight = torch.ones(self.num_classes)\n",
    "        empty_weight[0] = self.eos_coef\n",
    "        self.register_buffer('empty_weight', empty_weight)\n",
    "\n",
    "#03/21 debug        print(\"weight_vector:\")\n",
    "#        print(empty_weight.())\n",
    "\n",
    "    def loss_labels(self, outputs, targets, indices, num_points):\n",
    "        \"\"\"Classification loss (NLL)\n",
    "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
    "        \"\"\"\n",
    "        assert 'pred_logits' in outputs\n",
    "        src_logits = outputs['pred_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(src_logits.shape[:2], 0,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o.squeeze()\n",
    "\n",
    "        torch.set_printoptions(edgeitems=1000)\n",
    "#03/21 debug        print(\"logits\")\n",
    "#03/21 debug        print(src_logits.transpose(1, 2).size())\n",
    "#03/21 debug        print(\"target_class:\")\n",
    "#03/21 debug        print(target_classes.size())\n",
    "#03/21 debug        print(\"weight:\")\n",
    "#03/21 debug        print(self.empty_weight.size())\n",
    "\n",
    "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
    "        losses = {'loss_ce': loss_ce}\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def loss_points(self, outputs, targets, indices, num_points):\n",
    "\n",
    "        assert 'pred_points' in outputs\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_points = outputs['pred_points'][idx]\n",
    "        target_points = torch.cat([t['point'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "        loss_bbox = F.mse_loss(src_points, target_points, reduction='none')\n",
    "\n",
    "        losses = {}\n",
    "        losses['loss_points'] = loss_bbox.sum() / num_points\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        # permute targets following indices\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "\n",
    "    def get_loss(self, loss, outputs, targets, indices, num_points, **kwargs):\n",
    "        loss_map = {\n",
    "            'labels': self.loss_labels,\n",
    "            'points': self.loss_points,\n",
    "        }\n",
    "#03/21 debug        print(\"loss_map {}\".format(loss_map))\n",
    "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
    "        return loss_map[loss](outputs, targets, indices, num_points, **kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" This performs the loss computation.\n",
    "        Parameters:\n",
    "             outputs: dict of tensors, see the output specification of the model for the format\n",
    "             targets: list of dicts, such that len(targets) == batch_size.\n",
    "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
    "        \"\"\"\n",
    "        output1 = {'pred_logits': outputs['pred_logits'], 'pred_points': outputs['pred_points']}\n",
    "        indices1 = self.matcher(output1, targets)\n",
    "\n",
    "        num_points = sum(len(t[\"labels\"]) for t in targets)\n",
    "\n",
    "        num_points = torch.as_tensor([num_points], dtype=torch.float, device=next(iter(output1.values())).device)\n",
    "        if is_dist_avail_and_initialized():\n",
    "            torch.distributed.all_reduce(num_points)\n",
    "        #\n",
    "        num_boxes = torch.clamp(num_points / get_world_size(), min=1).item()\n",
    "\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            losses.update(self.get_loss(loss, output1, targets, indices1, num_boxes))\n",
    "\n",
    "        return losses\n",
    "\n",
    "# create the P2PNet model\n",
    "def build(args, training):\n",
    "    # treats persons as a single class\n",
    "    num_classes = label_type_count\n",
    "\n",
    "    model = P2PNet(args.row, args.line)\n",
    "    if not training:\n",
    "        return model\n",
    "\n",
    "    weight_dict = {'loss_ce': label_type_count, 'loss_points': args.point_loss_coef}\n",
    "#    weight_dict = {'loss_ce': 1, 'loss_points': args.point_loss_coef}  2023/03/19 バグ取り\n",
    "    losses = ['labels', 'points']\n",
    "    matcher = build_matcher_crowd(args)\n",
    "    criterion = SetCriterion_Crowd(num_classes, \\\n",
    "                                matcher=matcher, weight_dict=weight_dict, \\\n",
    "                                eos_coef=args.eos_coef, losses=losses)\n",
    "\n",
    "#03/21 debug    print(\"criterion:\")\n",
    "#03/21 debug    print(criterion)\n",
    "    return model, criterion\n",
    "###\n",
    "# create the P2PNet model for execution\n",
    "def build_eval(args):\n",
    "    # treats persons as a single class\n",
    "    num_classes = label_type_count\n",
    "\n",
    "    model = P2PNet(args.row, args.line)\n",
    "    return model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_arguments():\n",
    "    \"\"\"Parse all the arguments provided from the CLI.\n",
    "    Returns:\n",
    "      A list of parsed arguments.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #Change into rice data !!! -> chage default value\n",
    "    parser = argparse.ArgumentParser(description=\"Object Counting Framework\")\n",
    "    # constant\n",
    "    parser.add_argument('--lr', default=learning_rate, type=float) # originall set 1e-3 and can be reduced at later training stage\n",
    "    parser.add_argument('--lr_fpn', default=learning_rate_2, type=float)\n",
    "    parser.add_argument('--batch_size', default=1, type=int)\n",
    "#    parser.add_argument('--batch_size', default=4, type=int)\n",
    "\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=100, type=int)\n",
    "    parser.add_argument('--lr_drop', default=2000000, type=int)\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float, help='gradient clipping max norm')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None, help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
    "\n",
    "    # * Matcher is a Hungarian strategy, minimizing the cost of proposed points and gt points\n",
    "    parser.add_argument('--set_cost_class', default=0.5, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\") #クラスマッチングの失敗による重みづけ係数\n",
    "\n",
    "    parser.add_argument('--set_cost_point', default=0.5, type=float,\n",
    "                        help=\"L1 point coefficient in the matching cost\")\n",
    "\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--point_loss_coef', default=0.02, type=float) # default = 0.0002 #Change into rice data !!! -> chage default value\n",
    "    parser.add_argument('--eos_coef', default=0.01, type=float, # default 0.05, should be set low at the beginning !\n",
    "                        help=\"Relative classification weight of the no-object class\") # default = 0.5#Change into rice data !!! -> chage default value\n",
    "\n",
    "    # a threshold during evaluation for counting and visualization\n",
    "    parser.add_argument('--threshold', default=0.5, type=float,\n",
    "                        help=\"threshold in evalluation: evaluate_crowd_no_overlap\")#Change into rice data !!! -> chage default value\n",
    "    parser.add_argument('--row', default=2, type=int,\n",
    "                        help=\"row number of anchor points\")#Change into rice data !!! -> chage default value\n",
    "    parser.add_argument('--line', default=2, type=int,\n",
    "                        help=\"line number of anchor points\")#Change into rice data !!! -> chage default value\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', default='SHHA')\n",
    "    parser.add_argument('--data_root', default=training_data_root_dir,\n",
    "                        help='path where the dataset is')#Change into rice data !!! -> chage default value\n",
    "\n",
    "    parser.add_argument('--output_dir', default=inference_output_path,\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--checkpoints_dir', default = program_files_root_dir + 'CrowdCounting-P2PNet/MultiLevelPyramidFeature_01',\n",
    "                        help='path where to save checkpoints, empty for no saving') #ckpt_5n was not bad, default 2 X 2#Change into rice data !!! -> chage default value\n",
    "    parser.add_argument('--tensorboard_dir', default=program_files_root_dir + 'CrowdCounting-P2PNet/Grain_runs',\n",
    "                        help='path where to save, empty for no saving')#Change into rice data !!! -> chage default value\n",
    "\n",
    "    parser.add_argument('--seed', default=42, type=int)#Change into rice data !!! -> chage default value\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')#Change into rice data !!! -> chage default value\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')#Change into rice data !!! -> chage default value\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=1, type=int)\n",
    "    parser.add_argument('--eval_freq', default=5, type=int,\n",
    "                        help='frequency of evaluation, default setting is evaluating in every 5 epoch')#Change into rice data !!! -> chage default value\n",
    "    parser.add_argument('--gpu_id', default=0, type=int, help='the gpu used for training')\n",
    "    #\n",
    "    opt = parser.parse_known_args()[0] #if known else parser.parse_args()\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(lr=0.001, lr_fpn=0.0001, batch_size=1, weight_decay=0.0001, epochs=100, lr_drop=2000000, clip_max_norm=0.1, frozen_weights=None, set_cost_class=0.5, set_cost_point=0.5, point_loss_coef=0.02, eos_coef=0.01, threshold=0.5, row=2, line=2, dataset_file='SHHA', data_root='/home/hwang/d/GoogleDrive/howcanoewang@gmail.com/SharedWithMe/GrainCounting/AnalysisData_small/', output_dir='./outputs/exp_v2', checkpoints_dir='/home/hwang/d/GoogleDrive/howcanoewang@gmail.com/SharedWithMe/GrainCounting/programs/CrowdCounting-P2PNet/MultiLevelPyramidFeature_01', tensorboard_dir='/home/hwang/d/GoogleDrive/howcanoewang@gmail.com/SharedWithMe/GrainCounting/programs/CrowdCounting-P2PNet/Grain_runs', seed=42, resume='', start_epoch=0, eval=False, num_workers=1, eval_freq=5, gpu_id=0, vis_dir='./outputs/exp_v2/visual_pyramid')\n"
     ]
    }
   ],
   "source": [
    "train_args = get_train_arguments()\n",
    "#\n",
    "train_args.resume = \"\"\n",
    "\n",
    "# added by myself.hwang\n",
    "train_args.frozen_weights = None\n",
    "\n",
    "# args.vis_dir = inference_output_path + \"/Visualization_Soybean_MultiLevelPyramidFeature_out\"\n",
    "# train_args.vis_dir = inference_output_path + \"/visual_pyramid\"\n",
    "train_args.vis_dir = vis_dir\n",
    "# if not os.path.exists(train_args.vis_dir):\n",
    "#     os.makedirs(train_args.vis_dir)\n",
    "\n",
    "##ログファイルに記録する\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '{}'.format(train_args.gpu_id)\n",
    "\n",
    "# create the logging file\n",
    "train_args.output_dir = inference_output_path\n",
    "if not os.path.exists(train_args.output_dir):\n",
    "    os.makedirs(train_args.output_dir)\n",
    "\n",
    "run_log_name = os.path.abspath( os.path.join(inference_output_path, 'train_args.txt') )\n",
    "with open(run_log_name, \"w\") as log_file:\n",
    "    log_file.write('Eval Log %s\\n' % time.strftime(\"%c\"))\n",
    "\n",
    "# backup the arguments\n",
    "print(train_args)\n",
    "with open(run_log_name, \"a\") as log_file:\n",
    "    log_file.write(\"{}\".format(train_args))\n",
    "\n",
    "train_args.checkpoints_dir = checkpoints_dir\n",
    "best_model_file = os.path.join(train_args.checkpoints_dir, 'best_mae.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_list = []\n",
    "solid_path = '/content/drive/MyDrive/research/tanashi2022/GrainCounting/AnalysisData_small/'\n",
    "\n",
    "with open(training_list_path, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        # one line: `path/img_name.JPG,path/img_name.txt`\n",
    "        split_content = line.split(',')\n",
    "\n",
    "        new_split_content = [item.replace(solid_path, training_data_root_dir ).replace('\\n', '') for item in split_content]\n",
    "\n",
    "        train_img_list.append( new_split_content )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_img_list = []\n",
    "solid_path = '/content/drive/MyDrive/research/tanashi2022/GrainCounting/AnalysisData_small/'\n",
    "\n",
    "with open(evaluating_list_path, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        # one line: `path/img_name.JPG,path/img_name.txt`\n",
    "        split_content = line.split(',')\n",
    "\n",
    "        new_split_content = [item.replace(solid_path, training_data_root_dir ).replace('\\n', '') for item in split_content]\n",
    "\n",
    "        eval_img_list.append( new_split_content )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 15053426\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "seed = train_args.seed + utils.get_rank()\n",
    "seed = train_args.seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "# get the P2PNet model\n",
    "model, criterion = build(train_args, training=True)\n",
    "\n",
    "# move to GPU\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "model_without_ddp = model\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n",
    "# use different optimation params for different parts of the model\n",
    "param_dicts = [\n",
    "    {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"fpn\" not in n and p.requires_grad]},\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if \"fpn\" in n and p.requires_grad],\n",
    "        \"lr\": train_args.lr_fpn,\n",
    "    },\n",
    "]\n",
    "# Adam is used by default\n",
    "optimizer = torch.optim.Adam(param_dicts, lr=train_args.lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, train_args.lr_drop)\n",
    "\n",
    "# create the training and valiation set\n",
    "# train_set, val_set = loading_data(args.data_root)  # args_dataroot = training_data_root_dir\n",
    "train_set, val_set = loading_data( training_data_root_dir )\n",
    "\n",
    "# create the sampler used during training\n",
    "sampler_train = torch.utils.data.RandomSampler(train_set)\n",
    "sampler_val = torch.utils.data.SequentialSampler(val_set)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "    sampler_train, train_args.batch_size, drop_last=True)\n",
    "# the dataloader for training\n",
    "data_loader_train = DataLoader(train_set, batch_sampler=batch_sampler_train,\n",
    "                                collate_fn=utils.collate_fn_crowd, num_workers=train_args.num_workers)\n",
    "\n",
    "data_loader_val = DataLoader(val_set, 1, sampler=sampler_val,\n",
    "                                drop_last=False, collate_fn=utils.collate_fn_crowd, num_workers=train_args.num_workers)\n",
    "\n",
    "if train_args.frozen_weights is not None:\n",
    "    # checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "    checkpoint = torch.load(train_args.frozen_weights, weights_only=False, map_location=device)\n",
    "    model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "# resume the weights and training state if exists\n",
    "if train_args.resume:\n",
    "    print(\"resume\")\n",
    "    # checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    checkpoint = torch.load(train_args.resume, weights_only=False, map_location=device)\n",
    "    model_without_ddp.load_state_dict(checkpoint['model'], strict=False)\n",
    "    if not train_args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "      optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "      lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "      train_args.start_epoch = checkpoint['epoch'] + 1\n",
    "      print(checkpoint['epoch'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "[np.float64(5.4103525881470365)]\n",
      "[np.float64(5.641981743102097)]\n",
      "=======================================test=======================================\n",
      "mae: 5.4103525881470365 mse: 5.641981743102097 time: 3.422203302383423 best mse: 5.641981743102097\n",
      "=======================================test=======================================\n",
      "Averaged stats: lr: 0.001000  loss: 0.1790 (0.3241)  loss_ce: 0.1730 (0.3002)  loss_points: 0.0113 (0.0239)  loss_ce_unscaled: 0.0577 (0.1001)  loss_points_unscaled: 0.5647 (1.1973)\n",
      "[ep 0][lr 0.0010000][26.27s]\n",
      "Averaged stats: lr: 0.001000  loss: 0.1288 (0.2541)  loss_ce: 0.1074 (0.2248)  loss_points: 0.0158 (0.0294)  loss_ce_unscaled: 0.0358 (0.0749)  loss_points_unscaled: 0.7913 (1.4680)\n",
      "[ep 1][lr 0.0010000][25.89s]\n",
      "Averaged stats: lr: 0.001000  loss: 0.0869 (0.2199)  loss_ce: 0.0683 (0.2038)  loss_points: 0.0141 (0.0160)  loss_ce_unscaled: 0.0228 (0.0679)  loss_points_unscaled: 0.7066 (0.8023)\n",
      "[ep 2][lr 0.0010000][25.97s]\n",
      "Averaged stats: lr: 0.001000  loss: 0.1686 (0.2118)  loss_ce: 0.1478 (0.1965)  loss_points: 0.0144 (0.0153)  loss_ce_unscaled: 0.0493 (0.0655)  loss_points_unscaled: 0.7197 (0.7662)\n",
      "[ep 3][lr 0.0010000][25.87s]\n",
      "Averaged stats: lr: 0.001000  loss: 0.1009 (0.2074)  loss_ce: 0.0924 (0.1921)  loss_points: 0.0104 (0.0153)  loss_ce_unscaled: 0.0308 (0.0640)  loss_points_unscaled: 0.5181 (0.7647)\n",
      "[ep 4][lr 0.0010000][25.86s]\n",
      "===updated best model===\n",
      "[np.float64(5.4103525881470365), np.float64(4.0525131282820706)]\n",
      "[np.float64(5.641981743102097), np.float64(4.40878206989583)]\n",
      "=======================================test=======================================\n",
      "mae: 4.0525131282820706 mse: 4.40878206989583 time: 4.164269924163818 best mse: 4.40878206989583\n",
      "=======================================test=======================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     57\u001b[39m         step += \u001b[32m1\u001b[39m\n\u001b[32m     61\u001b[39m t1 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m stat = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_max_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# record the training states after every epoch\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/d/PortableSSD/hwang_Pro/jupyter/17_yamagishi_work/GrainCountingNet/jupyters/../gcp2pnet/engine.py:117\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, criterion, data_loader, optimizer, device, epoch, max_norm)\u001b[39m\n\u001b[32m    114\u001b[39m     optimizer.step()\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# update logger\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[43mmetric_logger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mloss_dict_reduced_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mloss_dict_reduced_unscaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m     metric_logger.update(lr=optimizer.param_groups[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# gather the stats from all processes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/d/PortableSSD/hwang_Pro/jupyter/17_yamagishi_work/GrainCountingNet/jupyters/../gcp2pnet/misc.py:170\u001b[39m, in \u001b[36mMetricLogger.update\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items():\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         v = \u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m))\n\u001b[32m    172\u001b[39m     \u001b[38;5;28mself\u001b[39m.meters[k].update(v)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "\n",
    "# the logger writer\n",
    "writer = SummaryWriter(train_args.tensorboard_dir)\n",
    "\n",
    "\n",
    "# save the performance during the training\n",
    "mae = []\n",
    "mse = []\n",
    "\n",
    "# threshold for evaluation\n",
    "threshold = train_args.threshold\n",
    "#\n",
    "step = 0\n",
    "# training starts here\n",
    "for epoch in range(train_args.start_epoch, train_args.epochs):\n",
    "\n",
    "    # run evaluation\n",
    "    if epoch % train_args.eval_freq == 0:\n",
    "        t1 = time.time()\n",
    "        result = evaluate_crowd_no_overlap(model, data_loader_val, device, class_n=2, threshold=threshold)\n",
    "        t2 = time.time()\n",
    "\n",
    "\n",
    "        # save the best model since begining\n",
    "        if epoch > 0:\n",
    "            if np.min(mse) > result[1]:\n",
    "            # if abs(np.min(mae) - result[0]) < 0.01:\n",
    "                checkpoint_best_path = best_model_file\n",
    "                torch.save({\n",
    "                    'model': model_without_ddp.state_dict(),\n",
    "                }, checkpoint_best_path)\n",
    "                print(\"===updated best model===\")\n",
    "\n",
    "        mae.append(result[0])\n",
    "        mse.append(result[1])\n",
    "\n",
    "        print(mae)\n",
    "        print(mse)\n",
    "\n",
    "        # print the evaluation results\n",
    "        print('=======================================test=======================================')\n",
    "#          print(\"mae:\", result[0], \"mse:\", result[1], \"time:\", t2 - t1, \"best mae:\", np.min(mae), )\n",
    "        print(\"mae:\", result[0], \"mse:\", result[1], \"time:\", t2 - t1, \"best mse:\", np.min(mse), )\n",
    "        with open(run_log_name, \"a\") as log_file:\n",
    "#              log_file.write(\"mae:{}, mse:{}, time:{}, best mae:{}\".format(result[0],result[1], t2 - t1, np.min(mae)))\n",
    "            log_file.write(\"mae:{}, mse:{}, time:{}, best mse:{}\".format(result[0],result[1], t2 - t1, np.min(mse)))\n",
    "        print('=======================================test=======================================')\n",
    "        # recored the evaluation results\n",
    "        if writer is not None:\n",
    "            with open(run_log_name, \"a\") as log_file:\n",
    "                log_file.write(\"metric/mae@{}: {}\".format(step, result[0]))\n",
    "                log_file.write(\"metric/mse@{}: {}\".format(step, result[1]))\n",
    "            writer.add_scalar('metric/mae', result[0], step)\n",
    "            writer.add_scalar('metric/mse', result[1], step)\n",
    "            step += 1\n",
    "\n",
    "\n",
    "\n",
    "    t1 = time.time()\n",
    "    stat = train_one_epoch(model, criterion, data_loader_train, optimizer, device, epoch, train_args.clip_max_norm)\n",
    "\n",
    "    # record the training states after every epoch\n",
    "    if writer is not None:\n",
    "        with open(run_log_name, \"a\") as log_file:\n",
    "            log_file.write(\"loss/loss@{}: {}\".format(epoch, stat['loss']))\n",
    "            log_file.write(\"loss/loss_ce@{}: {}\".format(epoch, stat['loss_ce']))\n",
    "\n",
    "        writer.add_scalar('loss/loss', stat['loss'], epoch)\n",
    "        writer.add_scalar('loss/loss_ce', stat['loss_ce'], epoch)\n",
    "\n",
    "    t2 = time.time()\n",
    "    print('[ep %d][lr %.7f][%.2fs]' % \\\n",
    "            (epoch, optimizer.param_groups[0]['lr'], t2 - t1))\n",
    "    with open(run_log_name, \"a\") as log_file:\n",
    "        log_file.write('[ep %d][lr %.7f][%.2fs]' % (epoch, optimizer.param_groups[0]['lr'], t2 - t1))\n",
    "    # change lr according to the scheduler\n",
    "    lr_scheduler.step()\n",
    "    # save latest weights every epoch\n",
    "    if not os.path.exists(train_args.checkpoints_dir):\n",
    "        os.makedirs(train_args.checkpoints_dir)\n",
    "    checkpoint_latest_path = os.path.join(train_args.checkpoints_dir, 'latest.pth')\n",
    "    torch.save({\n",
    "        'model': model_without_ddp.state_dict(),\n",
    "    }, checkpoint_latest_path)\n",
    "    if epoch % 300 == 0 and epoch != 0:\n",
    "        clear_output()\n",
    "# total time for training\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
