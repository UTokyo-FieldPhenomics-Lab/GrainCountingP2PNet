{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just the dataset differents from `gcp2pnet.train.main()`\n",
    "\n",
    "Train successfully, \n",
    "\n",
    "potential problem: `args.frozen_weight=None` setting or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from scipy import spatial\n",
    "import networkx as nx\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import glob\n",
    "import scipy.io as io\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import gcp2pnet\n",
    "\n",
    "from gcp2pnet.utils import DeNormalize, gen_random_scale_n\n",
    "from gcp2pnet.engine import train_one_epoch, evaluate_crowd_no_overlap\n",
    "\n",
    "# import gcp2pnet.misc as utils\n",
    "\n",
    "from gcp2pnet.models.attention import SpatialAttention, ChannelwiseAttention\n",
    "\n",
    "from gcp2pnet.models.pyrimid import (\n",
    "    vgg_conv1_2, vgg_conv2_2, vgg_conv3_3, vgg_conv4_3, vgg_conv5_3,\n",
    "    conv_1_2_hook, conv_2_2_hook, conv_3_3_hook, conv_4_3_hook, conv_5_3_hook,\n",
    "    CPFE, SODModel, \n",
    ")\n",
    "\n",
    "from gcp2pnet.models.matcher import HungarianMatcher_Crowd\n",
    "\n",
    "from gcp2pnet.models.p2pnet import (\n",
    "    RegressionModel, ClassificationModel, \n",
    "    generate_anchor_points, shift, \n",
    "    AnchorPoints, P2PNet, SetCriterion_Crowd, \n",
    "    build, build_eval,\n",
    ")\n",
    "\n",
    "from gcp2pnet.train import get_train_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimming_size               = 256\n",
    "padding_size                = 0\n",
    "learning_rate               = 1e-3 #learning rate for backgrond model\n",
    "learning_rate_2             = 1e-4 #learning rate for detection head\n",
    "\n",
    "label_dict = {'Fill': 1, '平べったいけど沈む': 1, '平べったくて浮く': 2, '詰まっているけど浮く': 2, 'Unfill': 2} \n",
    "label_type_count = 2+1\n",
    "\n",
    "ROOT_DIR = r\"/home/hwang/d/GoogleDrive/howcanoewang@gmail.com/SharedWithMe/GrainCounting\"\n",
    "\n",
    "training_data_root_dir      = f\"{ROOT_DIR}/AnalysisData_small/\"\n",
    "training_data_root_dir2     = f\"{ROOT_DIR}/AnalysisData/\"\n",
    "program_files_root_dir      = f\"{ROOT_DIR}/programs/\"\n",
    "net_code_dir                = f\"{program_files_root_dir}/CrowdCounting-P2PNet\"\n",
    "best_model_file             = f\"{net_code_dir}/MultiLevelPyramidFeature_01/best_mae.pth\"\n",
    "\n",
    "####下記フォルダに、学習させたい画像と、Annotationデータをセットで配置します\n",
    "\n",
    "training_set_root_data_dir                          = training_data_root_dir2 + \"SetRootData/\"  #こちらの各ディレクトリにjsonと画像ファイルを配置する\n",
    "\n",
    "training_data_images_dir_before_trimming            = training_data_root_dir + \"images/ImageBeforeTrimming/\" #こちらに画像を配置する\n",
    "evaluation_data_images_dir_before_trimming          = training_data_root_dir + \"evaluation/ImageBeforeTrimming/\" #こちらに画像を配置する\n",
    "inference_input_path_before_trimming                = training_data_root_dir + \"inference/Input/image_before_trimming\" #こちらに画像を配置する\n",
    "\n",
    "training_data_training_label_dir                    = training_data_root_dir + \"labels_Training\" #こちらにjsonファイルを配置する\n",
    "training_data_evaluation_label_dir                  = training_data_root_dir + \"labels_Evaluation\" #こちらにjsonファイルを配置する\n",
    "\n",
    "####学習用パッチ画像を生成すると更新される\n",
    "training_data_images_dir                            = training_data_root_dir + \"images/\" #こちらにパッチ画像が生成される\n",
    "evaluation_data_images_dir                          = training_data_root_dir + \"evaluation/\" #こちらにパッチ画像が生成される\n",
    "\n",
    "training_data_label_dir_before_trimming             = training_data_root_dir + \"labels_Training/only_coordinate/ImageBeforeTrimming/\" #こちらにjsonを変換したtxtが生成される\n",
    "evaluation_data_label_dir_before_trimming           = training_data_root_dir + \"labels_Evaluation/only_coordinate/ImageBeforeTrimming/\" #こちらにjsonを変換したtxtが生成される\n",
    "\n",
    "training_data_points_dir                            = training_data_root_dir + \"labels_Training/only_coordinate/\" #こちらにtxtファイルをパッチサイズで切り出したファイルが生成される\n",
    "evaluation_data_points_dir                          = training_data_root_dir + \"labels_Evaluation/only_coordinate/\" #こちらにtxtファイルをパッチサイズで切り出したファイルが生成される\n",
    "\n",
    "training_list_path                                  = training_data_root_dir + \"training_list.txt\" #こちらに画像⇔txtの対応ファイルが生成される\n",
    "evaluating_list_path                                = training_data_root_dir + \"evaluation_list.txt\" #こちらに画像⇔txtの対応ファイルが生成される\n",
    "\n",
    "#推論用パッチ画像を生成すると更新される\n",
    "# inference_input_path                                = training_data_root_dir + \"inference/Input\"#こちらにパッチ画像が生成される\n",
    "# inference_input_path                                = \"../data/inference\"\n",
    "# inference_output_path                               = training_data_root_dir + \"inference/Output\"#こちらに推論結果が生成される\n",
    "# inference_output_path                               = \"./outputs/exp_v3\"\n",
    "\n",
    "inference_integrated_image_file_path                = training_data_root_dir + \"inference/Output/Integrated\"#パッチ統合後画像が生成される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# datasets.py\n",
    "###############\n",
    "\n",
    "class SHHA(Dataset):\n",
    "    def __init__(self, data_root, transform=None, train=False, patch=False, flip=False):\n",
    "        self.root_path = data_root\n",
    "\n",
    "        #############################\n",
    "        #   Global variables DANGER #\n",
    "        #############################\n",
    "        self.train_lists = training_list_path ## rice grainsの場合\n",
    "        self.eval_lists = evaluating_list_path ## rice grainsの場合\n",
    "\n",
    "        # there may exist multiple list files\n",
    "        # e.g. \n",
    "        # ```\n",
    "        # content/drive/MyDrive/research/tanashi2022/GrainCounting/AnalysisData_small/images/20230209_08_G_a_v05_h11.JPG,/content/drive/MyDrive/research/tanashi2022/GrainCounting/AnalysisData_small/labels_Training/only_coordinate/20230209_08_G_a_v05_h11.txt\n",
    "        # /content/drive/MyDrive/research/tanashi2022/GrainCounting/AnalysisData_small/images/20221227_07_R_v05_h01.JPG,/content/drive/MyDrive/research/tanashi2022/GrainCounting/AnalysisData_small/labels_Training/only_coordinate/20221227_07_R_v05_h01.txt\n",
    "        # if train:\n",
    "        #     self.img_list_file = [name.split(',') for name in open(self.train_lists).read().splitlines()]\n",
    "        # else:\n",
    "        #     self.img_list_file = [name.split(',') for name in open(self.eval_lists).read().splitlines()]\n",
    "\n",
    "        # self.img_list = self.img_list_file\n",
    "\n",
    "        if train:\n",
    "            self.img_list = train_img_list\n",
    "        else:\n",
    "            self.img_list = eval_img_list\n",
    "\n",
    "        # number of samples\n",
    "        self.nSamples = len(self.img_list)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.patch = patch\n",
    "        self.flip = flip\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert index <= len(self), 'index range error'\n",
    "\n",
    "\n",
    "        img_path = self.img_list[index][0]\n",
    "        gt_path = self.img_list[index][1]\n",
    "\n",
    "        # load image and ground truth\n",
    "        img, point, labels = load_data((img_path, gt_path), self.train)\n",
    "\n",
    "        # applu augumentation\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.train:\n",
    "            # data augmentation -> random scale\n",
    "            scale_range = [0.5, 1.3]\n",
    "            min_size = min(img.shape[1:])\n",
    "            scale = random.uniform(*scale_range)\n",
    "            # scale the image and points\n",
    "            if scale * min_size > trimming_size:\n",
    "                img = torch.nn.functional.upsample_bilinear(img.unsqueeze(0), scale_factor=scale).squeeze(0)\n",
    "                point *= scale\n",
    "        # random crop augumentaiton\n",
    "        if self.train and self.patch:\n",
    "            img, point,labels = random_crop(img, point, labels)\n",
    "\n",
    "            for i, _ in enumerate(point):  #03/21 debug\n",
    "                point[i] = torch.Tensor(point[i])\n",
    "                labels[i] = torch.Tensor(labels[i])\n",
    "\n",
    "        # random flipping\n",
    "        if random.random() > 0.5 and self.train and self.flip:\n",
    "            # random flip\n",
    "            img = torch.Tensor(img[:, :, :, ::-1].copy())\n",
    "\n",
    "            for i, _ in enumerate(point):\n",
    "                point[i][:, 0] = trimming_size - point[i][:, 0]\n",
    "\n",
    "        if not self.train:\n",
    "            point = [point]\n",
    "            labels = [labels]\n",
    "\n",
    "        img = torch.Tensor(img)\n",
    "        # pack up related infos\n",
    "        target = [{} for i in range(len(point))]\n",
    "\n",
    "        for i, _ in enumerate(point):  #03/21 debug\n",
    "            target[i]['point'] = torch.Tensor(point[i])\n",
    "\n",
    "            if len(labels[0]) > 1:\n",
    "                target[i]['labels'] = torch.Tensor(labels[i].flatten()).long()\n",
    "            else:\n",
    "                target[i]['labels'] = torch.Tensor(labels[i]).long()\n",
    "\n",
    "            image_id_1 = int(img_path.split('/')[-1].split('.')[0][5:7])\n",
    "            image_id_1 = torch.Tensor([image_id_1]).long()\n",
    "            #\n",
    "            image_id_2 = int(img_path.split('/')[-1].split('.')[0][5:7])\n",
    "            image_id_2 = torch.Tensor([image_id_2]).long()\n",
    "\n",
    "            target[i]['image_id_1'] = image_id_1\n",
    "            target[i]['image_id_2'] = image_id_2\n",
    "\n",
    "        return img, target\n",
    "\n",
    "\n",
    "def load_data(img_gt_path, train):\n",
    "    img_path, gt_path = img_gt_path\n",
    "\n",
    "    # load the images\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    if not img is None:\n",
    "        img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # load ground truth points\n",
    "    points = []\n",
    "    labels = []\n",
    "    #\n",
    "    pts = open(gt_path).read().splitlines()\n",
    "    for pt_0 in pts:\n",
    "        pt = eval(pt_0)\n",
    "        x = float(pt[0])#/2\n",
    "        y = float(pt[1])#/2\n",
    "        label = float(pt[2])\n",
    "        points.append([x, y])\n",
    "        labels.append([label])\n",
    "\n",
    "    return img, np.array(points), np.array(labels)\n",
    "\n",
    "\n",
    "# random crop augumentation\n",
    "def random_crop(img, den, labels, num_patch=1):\n",
    "    half_h = trimming_size\n",
    "    half_w = trimming_size\n",
    "    result_img = np.zeros([num_patch, img.shape[0], half_h, half_w])\n",
    "    result_den = []\n",
    "    result_label = []\n",
    "    # crop num_patch for each image\n",
    "    for i in range(num_patch):\n",
    "        start_h = random.randint(0, img.size(1) - half_h)\n",
    "        start_w = random.randint(0, img.size(2) - half_w)\n",
    "        end_h = start_h + half_h\n",
    "        end_w = start_w + half_w\n",
    "        # copy the cropped rect\n",
    "        result_img[i] = img[:, start_h:end_h, start_w:end_w]\n",
    "        # copy the cropped points\n",
    "        idx = (den[:, 0] >= start_w) & (den[:, 0] <= end_w) & (den[:, 1] >= start_h) & (den[:, 1] <= end_h)\n",
    "        # shift the corrdinates\n",
    "        record_den = den[idx]\n",
    "        record_label = labels[idx]\n",
    "        record_den[:, 0] -= start_w\n",
    "        record_den[:, 1] -= start_h\n",
    "\n",
    "        result_den.append(record_den)\n",
    "        result_label.append(record_label)\n",
    "\n",
    "    return result_img, result_den, result_label\n",
    "\n",
    "###############\n",
    "# load_data.py\n",
    "###############\n",
    "import torchvision.transforms as standard_transforms\n",
    "\n",
    "# DeNormalize used to get original images\n",
    "class DeNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return tensor\n",
    "\n",
    "def loading_data(data_root):\n",
    "    # the pre-proccssing transform\n",
    "    transform = standard_transforms.Compose([\n",
    "        standard_transforms.ToTensor(),\n",
    "        standard_transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    # create the training dataset\n",
    "    train_set = SHHA(data_root, train=True, transform=transform, patch=True, flip=True)\n",
    "    # create the validation dataset\n",
    "    val_set = SHHA(data_root, train=False, transform=transform)\n",
    "\n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_list = []\n",
    "solid_path = '/content/drive/MyDrive/research/tanashi2022/GrainCounting/AnalysisData_small/'\n",
    "\n",
    "with open(training_list_path, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        # one line: `path/img_name.JPG,path/img_name.txt`\n",
    "        split_content = line.split(',')\n",
    "\n",
    "        new_split_content = [item.replace(solid_path, training_data_root_dir ).replace('\\n', '') for item in split_content]\n",
    "\n",
    "        train_img_list.append( new_split_content )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_img_list = []\n",
    "solid_path = '/content/drive/MyDrive/research/tanashi2022/GrainCounting/AnalysisData_small/'\n",
    "\n",
    "with open(evaluating_list_path, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        # one line: `path/img_name.JPG,path/img_name.txt`\n",
    "        split_content = line.split(',')\n",
    "\n",
    "        new_split_content = [item.replace(solid_path, training_data_root_dir ).replace('\\n', '') for item in split_content]\n",
    "\n",
    "        eval_img_list.append( new_split_content )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = loading_data( training_data_root_dir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_train_arguments()\n",
    "\n",
    "args.frozen_weights = None\n",
    "args.output_dir = \"./outputs\"\n",
    "args.run_name = \"exp_v3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifed `train.main()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 15053426\n",
      "================================================== Training Arguments ==================================================\n",
      "                  lr: 0.001\n",
      "              lr_fpn: 0.0001\n",
      "          batch_size: 1\n",
      "        weight_decay: 0.0001\n",
      "              epochs: 100\n",
      "             lr_drop: 2000000\n",
      "       clip_max_norm: 0.1\n",
      "      frozen_weights: None\n",
      "      set_cost_class: 0.5\n",
      "      set_cost_point: 0.5\n",
      "     point_loss_coef: 0.02\n",
      "            eos_coef: 0.01\n",
      "           threshold: 0.5\n",
      "                 row: 2\n",
      "                line: 2\n",
      "      dataset_folder: ./data/dataset\n",
      "          output_dir: ./outputs\n",
      "            run_name: exp_v3\n",
      "                seed: 42\n",
      "              resume: \n",
      "         start_epoch: 0\n",
      "                eval: False\n",
      "         num_workers: 1\n",
      "           eval_freq: 5\n",
      "              gpu_id: 0\n",
      "     checkpoints_dir: outputs/exp_v3/weights\n",
      "             vis_dir: outputs/exp_v3\n",
      "     tensorboard_dir: outputs/exp_v3/tensorboard_logs\n",
      "              device: cuda\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '{}'.format(args.gpu_id)\n",
    "\n",
    "run_output_dir = Path(args.output_dir) / args.run_name\n",
    "run_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "weights_dir = run_output_dir / 'weights'\n",
    "weights_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_model_file = weights_dir / \"best_mae.pth\"\n",
    "\n",
    "args.checkpoints_dir = weights_dir\n",
    "args.vis_dir = run_output_dir\n",
    "\n",
    "args.tensorboard_dir = run_output_dir / \"tensorboard_logs\"\n",
    "args.tensorboard_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "latest_ckpt = weights_dir / 'latest.pth'\n",
    "\n",
    "if latest_ckpt.exists() and args.resume == '':\n",
    "    user_input = input(f\"Found previous checkpoints at [{latest_ckpt}], resume training [Y/Yes] or retrain [N/No]? \").lower()\n",
    "    if user_input in ['y', 'yes']:\n",
    "        args.resume = str(latest_ckpt)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args.device = device\n",
    "\n",
    "# create the logging file\n",
    "run_log_name =  run_output_dir / 'args.yaml'\n",
    "with open(run_log_name, 'w', encoding=\"utf-8\") as f:\n",
    "    yaml.dump({k: str(v) if isinstance(v, Path) else v for k, v in vars(args).items()}, f)\n",
    "\n",
    "gcp2pnet.utils.fix_seed(args.seed)\n",
    "\n",
    "# get the P2PNet model\n",
    "model, criterion = gcp2pnet.models.p2pnet.build(args, training=True)\n",
    "\n",
    "# move to GPU\n",
    "model.to(args.device)\n",
    "criterion.to(args.device)\n",
    "\n",
    "model_without_ddp = model\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('number of params:', n_parameters)\n",
    "# use different optimation params for different parts of the model\n",
    "param_dicts = [\n",
    "    {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"fpn\" not in n and p.requires_grad]},\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if \"fpn\" in n and p.requires_grad],\n",
    "        \"lr\": args.lr_fpn,\n",
    "    },\n",
    "]\n",
    "# Adam is used by default\n",
    "optimizer = torch.optim.Adam(param_dicts, lr=args.lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "\n",
    "# create the training and valiation set\n",
    "# train_set, val_set = loading_data(args.data_root)  # args_dataroot = training_data_root_dir\n",
    "# train_set, val_set = datasets.loading_dataset( args.dataset_folder )\n",
    "# label_dict, class_n = datasets.loading_label_dict( args.dataset_folder ) \n",
    "class_n = 2\n",
    "\n",
    "# create the sampler used during training\n",
    "sampler_train = torch.utils.data.RandomSampler(train_set)\n",
    "sampler_val = torch.utils.data.SequentialSampler(val_set)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "    sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "# the dataloader for training\n",
    "data_loader_train = DataLoader(train_set, batch_sampler=batch_sampler_train,\n",
    "                                collate_fn=gcp2pnet.misc.collate_fn_crowd, \n",
    "                                num_workers=args.num_workers)\n",
    "\n",
    "data_loader_val = DataLoader(val_set, 1, sampler=sampler_val,\n",
    "                                drop_last=False, collate_fn=gcp2pnet.misc.collate_fn_crowd, \n",
    "                                num_workers=args.num_workers)\n",
    "\n",
    "if args.frozen_weights is not None:\n",
    "    # checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "    checkpoint = torch.load(args.frozen_weights, weights_only=False, map_location=device)\n",
    "    model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "# resume the weights and training state if exists\n",
    "if args.resume:\n",
    "    print(\"Resume previous checkpoints, continue training\")\n",
    "    # checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    checkpoint = torch.load(args.resume, weights_only=False, map_location=device)\n",
    "    model_without_ddp.load_state_dict(checkpoint['model'], strict=False)\n",
    "    if not args.eval and \\\n",
    "        'optimizer' in checkpoint and \\\n",
    "        'lr_scheduler' in checkpoint and \\\n",
    "        'epoch' in checkpoint:\n",
    "\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        args.start_epoch = checkpoint['epoch'] + 1\n",
    "        print(checkpoint['epoch'] + 1)\n",
    "\n",
    "gcp2pnet.utils.print_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "mae list: [5.41035259]\n",
      "mse list: [5.64198174]\n",
      "=======================================test=======================================\n",
      "mae: 5.4103525881470365 mse: 5.641981743102097 time: 3.2907180786132812 best mse: 5.641981743102097\n",
      "Averaged stats: lr: 0.001000  loss: 0.1697 (0.4484)  loss_ce: 0.1581 (0.3588)  loss_points: 0.0116 (0.0896)  loss_ce_unscaled: 0.0527 (0.1196)  loss_points_unscaled: 0.5801 (4.4778)\n",
      "epoch: 0 with loss= 0.4484020658338469 loss_ce=0.3588462857419966\n",
      "[ep 0][lr 0.0010000][26.06s]\n",
      "Averaged stats: lr: 0.001000  loss: 0.1548 (0.2211)  loss_ce: 0.1431 (0.2080)  loss_points: 0.0117 (0.0130)  loss_ce_unscaled: 0.0477 (0.0693)  loss_points_unscaled: 0.5842 (0.6521)\n",
      "epoch: 1 with loss= 0.22108532990751634 loss_ce=0.20804372399563795\n",
      "[ep 1][lr 0.0010000][25.84s]\n",
      "Averaged stats: lr: 0.001000  loss: 0.1272 (0.2109)  loss_ce: 0.1126 (0.1977)  loss_points: 0.0145 (0.0132)  loss_ce_unscaled: 0.0375 (0.0659)  loss_points_unscaled: 0.7268 (0.6602)\n",
      "epoch: 2 with loss= 0.21090280221811572 loss_ce=0.1976985054587623\n",
      "[ep 2][lr 0.0010000][25.84s]\n",
      "Averaged stats: lr: 0.001000  loss: 0.2165 (0.2077)  loss_ce: 0.2053 (0.1945)  loss_points: 0.0118 (0.0132)  loss_ce_unscaled: 0.0684 (0.0648)  loss_points_unscaled: 0.5891 (0.6623)\n",
      "epoch: 3 with loss= 0.20773629137293737 loss_ce=0.19449068304125566\n",
      "[ep 3][lr 0.0010000][25.84s]\n",
      "Averaged stats: lr: 0.001000  loss: 0.1454 (0.2052)  loss_ce: 0.1260 (0.1921)  loss_points: 0.0132 (0.0132)  loss_ce_unscaled: 0.0420 (0.0640)  loss_points_unscaled: 0.6587 (0.6583)\n",
      "epoch: 4 with loss= 0.2052309703212363 loss_ce=0.1920643983613118\n",
      "[ep 4][lr 0.0010000][25.88s]\n",
      "===updated best model===\n",
      "mae list: [5.41035259 3.7651913 ]\n",
      "mse list: [5.64198174 4.14270932]\n",
      "=======================================test=======================================\n",
      "mae: 3.765191297824456 mse: 4.142709320013599 time: 4.064661741256714 best mse: 4.142709320013599\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m         step += \u001b[32m1\u001b[39m\n\u001b[32m     53\u001b[39m t1 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m stat = \u001b[43mgcp2pnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_max_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# record the training states after every epoch\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     58\u001b[39m     \u001b[38;5;66;03m# with open(run_log_name, \"a\") as log_file:\u001b[39;00m\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m#     log_file.write(\"loss/loss@{}: {}\".format(epoch, stat['loss']))\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m#     log_file.write(\"loss/loss_ce@{}: {}\".format(epoch, stat['loss_ce']))\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/d/PortableSSD/hwang_Pro/jupyter/17_yamagishi_work/GrainCountingNet/jupyters/../gcp2pnet/engine.py:88\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, criterion, data_loader, optimizer, device, epoch, max_norm)\u001b[39m\n\u001b[32m     85\u001b[39m targets = [{k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t.items()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# calc the losses\u001b[39;00m\n\u001b[32m     90\u001b[39m loss_dict = criterion(outputs, targets)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/d/PortableSSD/hwang_Pro/jupyter/17_yamagishi_work/GrainCountingNet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/d/PortableSSD/hwang_Pro/jupyter/17_yamagishi_work/GrainCountingNet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/d/PortableSSD/hwang_Pro/jupyter/17_yamagishi_work/GrainCountingNet/jupyters/../gcp2pnet/models/p2pnet.py:168\u001b[39m, in \u001b[36mP2PNet.forward\u001b[39m\u001b[34m(self, samples)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples: NestedTensor):\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# get the backbone features\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# forward the feature pyramid\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     features_fpn_l, features_fpn_h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# output = bach_size, channel, Height, Weight\u001b[39;00m\n\u001b[32m    170\u001b[39m     batch_size = features_fpn_l.size()[\u001b[32m0\u001b[39m]\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m#  put low level features for points regression\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/d/PortableSSD/hwang_Pro/jupyter/17_yamagishi_work/GrainCountingNet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/d/PortableSSD/hwang_Pro/jupyter/17_yamagishi_work/GrainCountingNet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/d/PortableSSD/hwang_Pro/jupyter/17_yamagishi_work/GrainCountingNet/jupyters/../gcp2pnet/models/pyrimid.py:165\u001b[39m, in \u001b[36mSODModel.forward\u001b[39m\u001b[34m(self, input_)\u001b[39m\n\u001b[32m    162\u001b[39m conv_12_feats = torch.mul(conv_12_feats, conv_12_sa)\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# Fused features\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m fused_features_l = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv_12_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m fused_features_h = torch.sigmoid(conv_345_feats)\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m#fused_feats_l = conv_12_feats\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m#fused_features_lh = torch.add(conv_12_feats, conv_345_feats)\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m#fused_features_lh = torch.sigmoid(fused_features_lh)\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m#fused_feats_h = conv_345_feats\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;66;03m#fused_feats_h = torch.sigmoid(fused_feats_h)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#######################\n",
    "print(\"Start training\")\n",
    "#######################\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# the logger writer\n",
    "writer = SummaryWriter(args.tensorboard_dir)\n",
    "\n",
    "# save the performance during the training\n",
    "mae = []\n",
    "mse = []\n",
    "step = 0\n",
    "\n",
    "# training starts here\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "\n",
    "    # run evaluation\n",
    "    if epoch % args.eval_freq == 0:\n",
    "        t1 = time.time()\n",
    "        result = gcp2pnet.engine.evaluate_crowd_no_overlap(model, data_loader_val, device, class_n, args.threshold)\n",
    "        t2 = time.time()\n",
    "\n",
    "\n",
    "        # save the best model since begining\n",
    "        if epoch > 0:\n",
    "            if np.min(mse) > result[1]:\n",
    "            # if abs(np.min(mae) - result[0]) < 0.01:\n",
    "                checkpoint_best_path = best_model_file\n",
    "                torch.save({\n",
    "                    'model': model_without_ddp.state_dict(),\n",
    "                }, checkpoint_best_path)\n",
    "                print(\"===updated best model===\")\n",
    "\n",
    "        mae.append(result[0])\n",
    "        mse.append(result[1])\n",
    "\n",
    "        print(\"mae list:\", np.asarray(mae))\n",
    "        print(\"mse list:\", np.asarray(mse))\n",
    "\n",
    "        # print the evaluation results\n",
    "        print('=======================================test=======================================')\n",
    "        print(\"mae:\", result[0], \"mse:\", result[1], \"time:\", t2 - t1, \"best mse:\", np.min(mse), )\n",
    "        # with open(run_log_name, \"a\") as log_file:\n",
    "            # log_file.write(\"mae:{}, mse:{}, time:{}, best mae:{}\".format(result[0],result[1], t2 - t1, np.min(mae)))\n",
    "            # log_file.write(\"mae:{}, mse:{}, time:{}, best mse:{}\".format(result[0],result[1], t2 - t1, np.min(mse)))\n",
    "        # recored the evaluation results\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('metric/mae', result[0], step)\n",
    "            writer.add_scalar('metric/mse', result[1], step)\n",
    "            step += 1\n",
    "\n",
    "    t1 = time.time()\n",
    "    stat = gcp2pnet.engine.train_one_epoch(model, criterion, data_loader_train, optimizer, device, epoch, args.clip_max_norm)\n",
    "\n",
    "    # record the training states after every epoch\n",
    "    if writer is not None:\n",
    "        # with open(run_log_name, \"a\") as log_file:\n",
    "        #     log_file.write(\"loss/loss@{}: {}\".format(epoch, stat['loss']))\n",
    "        #     log_file.write(\"loss/loss_ce@{}: {}\".format(epoch, stat['loss_ce']))\n",
    "        print(f\"epoch: {epoch} with loss= {stat['loss']} loss_ce={stat['loss_ce']}\")\n",
    "        writer.add_scalar('loss/loss', stat['loss'], epoch)\n",
    "        writer.add_scalar('loss/loss_ce', stat['loss_ce'], epoch)\n",
    "\n",
    "    t2 = time.time()\n",
    "    print('[ep %d][lr %.7f][%.2fs]' % (epoch, optimizer.param_groups[0]['lr'], t2 - t1))\n",
    "    # with open(run_log_name, \"a\") as log_file:\n",
    "    #     log_file.write('[ep %d][lr %.7f][%.2fs]' % (epoch, optimizer.param_groups[0]['lr'], t2 - t1))\n",
    "    \n",
    "    # change lr according to the scheduler\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # save latest weights every epoch\n",
    "    if not os.path.exists(args.checkpoints_dir):\n",
    "        os.makedirs(args.checkpoints_dir)\n",
    "    checkpoint_latest_path = os.path.join(args.checkpoints_dir, 'latest.pth')\n",
    "\n",
    "    torch.save({\n",
    "        'model': model_without_ddp.state_dict(),\n",
    "    }, checkpoint_latest_path)\n",
    "\n",
    "# total time for training\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
